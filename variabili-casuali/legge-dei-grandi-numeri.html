<!DOCTYPE html>
<html lang="IT">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="stylesheet" href="../styles/style.css" />
    <link rel="stylesheet" href="../styles/index-style.css" />
    <link rel="stylesheet" href="../styles/content-style.css" />
    <link rel="icon" type="image/x-icon" href="../resources/favicon.ico">
    <meta name="application-name" content="Matematica applicata" />
    <meta name="apple-mobile-web-app-title" content="Matematica applicata" />
    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="#E34234" />
    <link rel="apple-touch-icon" href="../resources/favicon.png" />
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-78NHLXDQD8"></script>
    <script src="../scripts/script.js"></script>
    <style>:root { --bg-clr: #E34234; --fg-clr: #262626; }</style>
    <meta name="theme-color" content="#E34234" />
    <meta name="keywords" content="matematica applicata" />
    <meta name="description" content="Il seguente sito contiene gli appunti e le definizioni del corso 'Matematica applicata'.">
    <meta name="robots" content="index">
    <script defer async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <title>Matematica applicata - Variabili casuali - Legge dei grandi numeri</title>
</head>
<body>
    <div class="container">
        <header class="header-wrapper">
            <div class="title-wrapper">
                <span class="title">
                    Matematica applicata
                </span>
                <span class="subtitle">
                    by lorenzoarlo
                </span>
            </div>
            <span class="page-title">
                Variabili casuali
            </span>
        </header>
        <section class="content-wrapper">
            <h1 class="section-title content-width">Legge dei grandi numeri</h1>
            <article class="content-container content-width">
                <div class="demonstration environment" id="dem2-10"><h2 class="environment-title">Dimostrazione - Disuguaglianza di Markov</h2><div class="environment-body">     Data la proposizione     <div class="proposition environment"><h3 class="environment-title">Enunciato</h3><div class="environment-body">         Considerando una variabile casuale <span class="math-span">\( X\)</span> a valori non negativi (<span class="math-span">\( X \geq 0\)</span>) ed un numero <span class="math-span">\( a \in \mathbb{R}^+\)</span> (ovvero <span class="math-span">\( a \gt 0\)</span>), si ha che         <span class="math-block">\[             P(X \gt a) \leq \frac{E[X]}{a}           \]</span></div></div><div class="proof environment"><h3 class="environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="toggle_proof(event)">visibility_off</span></h3><div class="environment-body hyde"><span class="inner-title">Caso di variabile continua</span>         Per dimostrare questa proposizione, consideriamo la definizione di valore atteso nel caso di variabili continue         <span class="math-block">\begin{aligned}             &amp; E[X] = \int_{-\infty}^{+\infty} x \cdot f(x) \ dx &amp; \iff         \end{aligned}</span>         Dato ciò, è possibile dividere l'integrale in due intervalli ed eliminare l'intervallo negativo (in quanto uguale a <span class="math-span">\( 0\)</span> per ipotesi), ovvero         <span class="math-block">\begin{aligned}             &amp; E[X] = \overbrace{\int_{-\infty}^{0} x \cdot f(x) \ dx}^0 + \int_{0}^{+\infty} x \cdot f(x) \ dx  &amp; \iff \\             &amp; E[X] = \int_{0}^{+\infty} x \cdot f(x) \ dx &amp; \iff         \end{aligned}</span>         Ora, sapendo che <span class="math-span">\( a \gt 0\)</span>, si ha che è possibile dividere ulteriormente l'integrale         <span class="math-block">\begin{aligned}             &amp; E[X] = \int_{0}^{a} x \cdot f(x) \ dx + \int_{a}^{+\infty} x \cdot f(x) \ dx &amp; \iff         \end{aligned}</span>         Considerato ciò, si ha che <span class="math-span">\( \int_{0}^{a} x \cdot f(x) \ dx \geq 0\)</span> in quanto <span class="math-span">\( X\)</span> è a valori non negativi e per questo motivo si ha che è possibile scrivere         <span class="math-block">\begin{aligned}             &amp; \underbrace{\int_{0}^{a} x \cdot f(x) \ dx + \int_{a}^{+\infty} x \cdot f(x) \ dx}_{E[X]} \geq \int_{a}^{+\infty} x \cdot f(x) \ dx &amp; \iff \\             &amp; E[X] \geq \int_{a}^{+\infty} x \cdot f(x) \ dx &amp; \iff         \end{aligned}</span>         Dato ora che nell'integrale <span class="math-span">\( x\)</span> può variare da <span class="math-span">\( a\)</span> a <span class="math-span">\( +\infty\)</span> (quindi si ha che sicuramente <span class="math-span">\( x \geq a\)</span>), si ha che è possibile scrivere         <span class="math-block">\begin{aligned}             &amp; E[X] \geq \int_{a}^{+\infty} x \cdot f(x) \ dx \geq \int_{a}^{+\infty} a \cdot f(x) &amp; \iff         \end{aligned}</span>         e quindi vale         <span class="math-block">\begin{aligned}             &amp; E[X] \geq a \cdot \int_{a}^{+\infty} f(x) &amp; \iff         \end{aligned}</span>         Quindi, dato che <span class="math-span">\( \int_{a}^{+\infty} f(x) = P(X \geq a)\)</span>, si ha che         <span class="math-block">\begin{aligned}             &amp; E[X] \geq a \cdot P(X \geq a) &amp; \iff \\             &amp; \frac{E[X]}{a} \geq P(X \geq a) &amp;         \end{aligned}</span>         che dimostra la proposizione.     </div></div></div></div><div class="demonstration environment" id="dem2-11"><h2 class="environment-title">Dimostrazione - Disuguaglianza di Cebycev</h2><div class="environment-body">     Data la proposizione     <div class="proposition environment"><h3 class="environment-title">Enunciato</h3><div class="environment-body">         Considerando una variabile casuale <span class="math-span">\( X\)</span> per cui esistono <span class="math-span">\( E[X] = \mu\)</span> e <span class="math-span">\( Var(X) = \sigma^2\)</span> e un numero <span class="math-span">\( r \in \mathbb{R}^+\)</span> (ovvero <span class="math-span">\( r \gt 0\)</span>), si ha che         <span class="math-block">\[             P(\left| X - \mu \right| \geq r) \leq \frac{\sigma^2}{r^2}             \]</span></div></div><div class="proof environment"><h3 class="environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="toggle_proof(event)">visibility_off</span></h3><div class="environment-body hyde">         Per dimostrare questa proposizione consideriamo che          <span class="math-block">\begin{aligned}             &amp; P(\left| X - \mu \right| \geq r) = P((X - \mu)^2 \geq r^2) &amp; \iff         \end{aligned}</span>         Consideriamo quindi la variabile casuale <span class="math-span">\( Y = (X - \mu)^2\)</span> che non è negativa, si ha che         <span class="math-block">\begin{aligned}             &amp; P((X - \mu)^2 \geq r^2) = P(Y \geq r^2) &amp; \iff          \end{aligned}</span>         e per la disuguaglianza di Markov si ha che         <span class="math-block">\begin{aligned}             &amp; P(Y \geq r^2) \leq \frac{E[Y]}{r^2} &amp; \iff         \end{aligned}</span>         Sostituendo ora <span class="math-span">\( Y = (X - \mu)^2\)</span> e ricordando la condizione iniziale, si ha che         <span class="math-block">\begin{aligned}             &amp; P(\left| X - \mu \right| \geq r) \leq \frac{E\left[(X - \mu)^2\right]}{r^2} &amp; \iff         \end{aligned}</span>         Ora, dato che <span class="math-span">\( E[(X - \mu)^2] = Var(X)\)</span> che è uguale al quadrato della deviazione standard, ovvero <span class="math-span">\( Var(X) = \sigma^2\)</span>, per cui         <span class="math-block">\[             P(\left| X - \mu \right| \geq r) \leq \frac{\sigma^2}{r^2}         \]</span>         che dimostra la proposizione.     </div></div><div class="mynote environment"><h3 class="environment-title">Osservazioni personali - In altre parole</h3><div class="environment-body">         Secondo la disuguaglianza di Chebicev, la probabilità che una variabile casuale <span class="math-span">\( X\)</span> "disti" dal suo valore medio più di <span class="math-span">\( r\)</span> è minore o uguale a <span class="math-span">\( \frac{\sigma^2}{r^2}\)</span>.     </div></div></div></div><div class="myexample environment" id="example31"><h2 class="environment-title">Esempio - Utilizzo delle disuguaglianze di Markov e Cebycev</h2><div class="environment-body collapsed">     Considerando un esame, il cui punteggio varia da <span class="math-span">\( 0\)</span> a <span class="math-span">\( 100\)</span>, si ha che uno studente a caso ottiene mediamente un punteggio di <span class="math-span">\( 75\)</span>.      Si sa inoltre che mediamente il voto varia di <span class="math-span">\( 25\)</span> punti dal valore medio.      Stimare:     <ul class="list-container"><li class="list-item">la probabilità massima che il punteggio non sia inferiore a <span class="math-span">\( 90\)</span>;         </li><li class="list-item">la probabilità massima che il punteggio sia <span class="math-span">\( \leq 65\)</span> o <span class="math-span">\( \geq 85\)</span>.     </li></ul><span class="inner-title">Modellare la realtà</span>     Considerando questo problema, si ha che è possibile considerare la variabile casuale <span class="math-span">\( X\)</span> come il punteggio all'esame. Avremo quindi che il valore atteso <span class="math-span">\( E[X]\)</span> sarà <span class="math-span">\( 75\)</span> e che la varianza di <span class="math-span">\( X\)</span> sia <span class="math-span">\( 25\)</span>.     <span class="inner-title">Probabilità massima che il punteggio non sia inferiore a <span class="math-span">\( 90\)</span></span>     Stimare la probabilità massima che il punteggio non sia inferiore a <span class="math-span">\( 90\)</span> (ovvero <span class="math-span">\( P(X \geq 90)\)</span>) equivale a cercare <span class="math-span">\( ?\)</span><span class="math-block">\[         P(X \geq 90) \leq \ ?         \]</span>     È quindi possibile utilizzare la disuguaglianza di Markov per cui si ha che     <span class="math-block">\[         \begin{array}{ccl}             P(X \geq 90) &amp; \leq &amp; \frac{E[X]}{90} \\             &amp; \leq &amp; \frac{75}{90} \\             &amp; \leq &amp; \frac{5}{6}         \end{array}       \]</span>     Si ha quindi che la probabilità massima che il punteggio non sia inferiore a <span class="math-span">\( 90\)</span> è <span class="math-span">\( \frac{5}{6}\)</span>.     <span class="inner-title">Probabilità che il punteggio sia <span class="math-span">\( \leq 65\)</span> o <span class="math-span">\( \geq 85\)</span></span>     Stimare la probabilità massima che il punteggio sia <span class="math-span">\( \leq 65\)</span> o <span class="math-span">\( \geq 85\)</span> (ovvero <span class="math-span">\( P(X \leq 65 \cup X \geq 85)\)</span>) equivale a calcolare la probabilità che <span class="math-span">\( X\)</span> "disti" dal valore atteso <span class="math-span">\( 75\)</span> di più di <span class="math-span">\( 10\)</span> punti, ovvero     <span class="math-block">\[         P(X \leq 65 \cup X \geq 85) = P(\left| X - 75 \right| \geq 10)      \]</span>     ma è possibile notare che <span class="math-span">\( 75\)</span> è il valore atteso per cui è possibile considerare la disuguaglianza di Chebicev, per cui     <span class="math-block">\[         \begin{array}{ccl}             P(\left| X - 75 \right| \geq 10) &amp; \leq &amp; \frac{\sigma^2}{10^2} \\             &amp; \leq &amp; \frac{25}{100} \\             &amp; \leq &amp; \frac{1}{4}         \end{array}     \]</span></div><div class="environment-tail"><span class="material-symbols-outlined body-visibility-icon" onclick="expand_environment(event)">expand_more</span></div></div><div class="demonstration environment" id="dem2-12"><h2 class="environment-title">Dimostrazione - Legge dei grandi numeri</h2><div class="environment-body">     Dato il teorema     <div class="proposition environment"><h3 class="environment-title">Enunciato</h3><div class="environment-body">         Data una successione di variabili casuali <span class="math-span">\( X_1, \ldots, X_n\)</span> indipendenti e identicamente distribuite (quindi con <span class="math-span">\( E[X_k] = \mu\)</span> e <span class="math-span">\( Var(X_k) = \sigma^2\)</span>), si ha che la probabilità della media di tali variabili converge al valore atteso <span class="math-span">\( \mu\)</span>, ovvero         <span class="math-block">\[             \lim_{n \to +\infty} P(\left| \overline{X} - \mu \right| \geq \varepsilon) = 0              \qquad \forall \varepsilon \in \mathbb{R}^+         \]</span>         con <span class="math-span">\( \overline{X} = \sum_{k = 1}^n \frac{X_k}{n}\)</span>.         <br/>         Ciò significa che la "distanza" tra la media <span class="math-span">\( \overline{X}\)</span> e il valore atteso <span class="math-span">\( \mu\)</span>, è nulla in quanto si ha che per ogni <span class="math-span">\( \varepsilon\)</span> (anche molto piccolo) è <span class="math-span">\( 0\)</span>.     </div></div><div class="mynote environment"><h3 class="environment-title">Osservazioni personali - In altre parole</h3><div class="environment-body">         La legge dei grandi numeri implica che, considerando un esperimento, si ha che ripetendo "molte" volte le misurazioni si ha una probabilità sempre maggiore che il valore della media degli esperimenti <span class="math-span">\( \overline{X}\)</span> sia uguale al valore teorico medio <span class="math-span">\( \mu\)</span>.      </div></div><div class="proof environment"><h3 class="environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="toggle_proof(event)">visibility_off</span></h3><div class="environment-body hyde"><span class="inner-title">Valore atteso di <span class="math-span">\( \overline{X}\)</span></span>         Per dimostrare questo teorema, consideriamo che tutte le variabili casuali sono identicamente distribuite, ovvero         <span class="math-block">\[             E[X_1] = \ldots = E[X_n] = \mu           \]</span>         si ha quindi che il valore atteso di <span class="math-span">\( \overline{X}\)</span>, definito come         <span class="math-block">\[             \overline{X} = \frac{\sum_{k = 1}^n X_k}{n}         \]</span>         sarà uguale a         <span class="math-block">\[             \begin{array}{ccl}                 E[\overline{X}] &amp; = &amp; E\left[ \frac{\sum_{k = 1}^n X_k}{n} \right] \\                 &amp; = &amp; \frac{1}{n} \cdot E\left[ \sum_{k = 1}^n X_k \right]  \\                 &amp; = &amp; \frac{1}{n} \cdot \sum_{k = 1}^n E[X_k] \\                 &amp; = &amp; \frac{1}{n} \cdot n \cdot \mu \\                 &amp; = &amp; \mu             \end{array}         \]</span><span class="inner-title">Legge dei grandi numeri</span>         Consideriamo ora che le variabili casuali sono indipendenti e ciò comporta che la covarianza sia nulla e si abbia         <span class="math-block">\[             Var(X + Y) = Var(X) + Var(Y) + \overbrace{2 \cdot Cov(X, Y)}^0         \]</span>         Per questo motivo avremo che         <span class="math-block">\[             \begin{array}{ccl}                 Var(\overline{X}) &amp; = &amp; Var\left( \frac{\sum_{k = 1}^n X_k}{n} \right) \\                 &amp; = &amp; \frac{1}{n^2} \cdot Var\left( \sum_{k = 1}^n X_k \right)  \\                 &amp; = &amp; \frac{1}{n^2} \underbrace{\cdot \sum_{k = 1}^n Var(X_k)}_{n \cdot \sigma^2} \\                 &amp; = &amp; \frac{1}{n^2} \cdot n \cdot \sigma^2 \\                 &amp; = &amp; \frac{\sigma^2}{n}             \end{array}         \]</span><span class="inner-title"><span class="math-span">\( P(\left| \overline{X} - \mu \right| \geq \varepsilon)\)</span></span>         Considendo che <span class="math-span">\( \mu = E[\overline{X}]\)</span>, si ha che         <span class="math-block">\begin{aligned}             &amp; P(\left| \overline{X} - \mu \right| \geq \varepsilon) = P(\left| \overline{X} - E[\overline{X}] \right| \geq \varepsilon) &amp; \iff          \end{aligned}</span>         ora, per la disuguaglianza di Chebicev, si ha che         <span class="math-block">\begin{aligned}             &amp; P(\left| \overline{X} - E[\overline{X}] \right| \geq \varepsilon) \leq \underbrace{\frac{Var(\overline{X})}{\varepsilon^2}}_{\frac{\sigma^2}{n \cdot \varepsilon^2}} &amp; \iff          \end{aligned}</span>         per cui si ha che         <span class="math-block">\begin{aligned}             &amp; P(\left| \overline{X} - E[\overline{X}] \right| \geq \varepsilon) \leq \frac{\sigma^2}{n \cdot \varepsilon^2} &amp; \iff          \end{aligned}</span>         Applicando ora il limite per <span class="math-span">\( n \to +\infty\)</span>, si ha che         <span class="math-block">\begin{aligned}             &amp; \lim_{n \to +\infty} P(\left| \overline{X} - E[\overline{X}] \right| \geq \varepsilon) \leq \underbrace{\lim_{n \to +\infty} \frac{\sigma^2}{n \cdot \varepsilon^2}}_0 &amp; \iff \\             &amp; \lim_{n \to +\infty} P(\left| \overline{X} - E[\overline{X}] \right| \geq \varepsilon) \leq 0 &amp;         \end{aligned}</span>         Ora, considerando gli assiomi di Kolmogorov, si ha che una probabilità non può essere nulla per cui abbiamo che         <span class="math-block">\[             \lim_{n \to +\infty} P(\left| \overline{X} - E[\overline{X}] \right| \geq \varepsilon) = 0          \]</span>         che dimostra il teorema.     </div></div><div class="mynote environment"><h3 class="environment-title">Osservazioni personali - Varianza nulla al crescere di <span class="math-span">\( n\)</span></h3><div class="environment-body">         Tale legge si basa anche sul fatto che al crescere di <span class="math-span">\( n\)</span> la varianza di <span class="math-span">\( \overline{X}\)</span> diventa nulla.         Come si è dimostrato, si ha infatti che          <span class="math-block">\[             Var(\overline{X}) = \frac{\sigma^2}{n}           \]</span>         che nel caso <span class="math-span">\( n \to +\infty\)</span> sarà uguale a <span class="math-span">\( 0\)</span>.     </div></div></div></div><div class="demonstration environment" id="dem2-13"><h2 class="environment-title">Dimostrazione - Corollario di Bernoulli</h2><div class="environment-body">     Dato il corollario     <div class="proposition environment"><h3 class="environment-title">Enunciato</h3><div class="environment-body">         Al tendere ad infinito del numero di tentativi, la definizione frequentista di probabilità di un evento <span class="math-span">\( A\)</span>, ovvero         <span class="math-block">\[             f_A = \frac{n^\circ \ \text{tentativi in cui $A$ si verifica}}{n^\circ \ \text{di tentativi svolti}}         \]</span>         si ha che converge alla probabilità tecnica di <span class="math-span">\( A\)</span>, ovvero         <span class="math-block">\[             \lim_{n \to +\infty} P(\left| f_A - P(A) \right| \geq \varepsilon) = 0 \qquad \forall \varepsilon \in \mathbb{R}^+            \]</span></div></div><div class="proof environment"><h3 class="environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="toggle_proof(event)">visibility_off</span></h3><div class="environment-body hyde">         Per dimostrare questo corollario consideriamo che l'esito di ogni tentativo (degli <span class="math-span">\( n\)</span> totali) sia rappresentato da una variabile casuale di Bernoulli di parametro <span class="math-span">\( p\)</span>, ovvero         <span class="math-block">\[             X_k \sim Be(p)  \qquad \forall k \in \{ 1, \ldots, n \}         \]</span>         si avrà quindi che il numero di tentativi in cui si è verificato l'evento <span class="math-span">\( A\)</span> (ovvero <span class="math-span">\( f_A\)</span>) sarà uguale a         <span class="math-block">\[             \overline{X} = \frac{\sum_{k = 1}^n X_k}{n} = f_A         \]</span>         considerando che sono variabili di Bernoulli, si ha che         <span class="math-block">\[             E[\overline{X}] = p = P(A)           \]</span>         Avremo quindi per la legge dei grandi numeri che         <span class="math-block">\begin{aligned}             &amp; \lim_{n \to +\infty} P(\left| \overline{X} - p \right| \geq \varepsilon) = 0 &amp; \iff \\             &amp; \lim_{n \to +\infty} P(\left| f_A - P(A) \right| \geq \varepsilon) = 0 &amp;         \end{aligned}</span>         che dimostra il corollario.     </div></div><div class="mynote environment"><h3 class="environment-title">Osservazioni personali - Quindi...</h3><div class="environment-body">         Questo corollario dimostra il fatto che la definizione frequentista di probabilità converge in probabilità alla probabilità teorica.     </div></div></div></div>
            </article>
            <nav class="buttons-container content-width">
                <a class="navigation-button previous" href="modelli-di-variabili-casuali-continue.html" rel="nofollow"><span>Modelli di variabili casuali continue</span></a>
                
            </nav>
        </section>
        <div class="scroll-to-bottom-button" onclick="scroll_to_bottom()">
            <span class="material-symbols-outlined">
                keyboard_double_arrow_down
            </span>
        </div>
        <footer class="footer-wrapper">
            <div class="copyright-wrapper">
                <span> &copy; Copyright 2023</span> /
                <span>made by lorenzoarlo</span>
            </div>
            /
            <div class="privacy-wrapper">
                <span><a href="https://lorenzoarlo.github.io/privacy-and-cookies/" rel="nofollow">Pannello preferenze cookie</a></span> /
                <span><a href="https://lorenzoarlo.github.io/privacy-and-cookies/privacy-policy.html" rel="nofollow" target="_blank" >Privacy Policy</a></span>
            </div>
        </footer>
    </div>
</body>
</html>