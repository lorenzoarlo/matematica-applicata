<!DOCTYPE html>
<html lang="IT">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="stylesheet" href="../styles/style.css" />
    <link rel="stylesheet" href="../styles/index-style.css" />
    <link rel="stylesheet" href="../styles/content-style.css" />
    <link rel="icon" type="image/x-icon" href="../resources/favicon.ico">
    <link rel="apple-touch-icon" href="../resources/favicon.png" />
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-78NHLXDQD8"></script>
    <script src="../scripts/script.js"></script>
    <style>:root { --bg-clr: #E34234; --fg-clr: #262626; }</style>
    <meta name="theme-color" content="#E34234" />
    <meta name="keywords" content="matematica applicata" />
    <meta name="description" content="Il seguente sito contiene gli appunti e le definizioni del corso 'Matematica applicata'.">
    <meta name="robots" content="index">
    <script defer async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <title>Matematica applicata - Variabili casuali - Covarianza</title>
</head>
<body>
    <div class="container">
        <header class="header-wrapper">
            <div class="title-wrapper">
                <span class="title">
                    Matematica applicata
                </span>
                <span class="subtitle">
                    by lorenzoarlo
                </span>
            </div>
            <span class="page-title">
                Variabili casuali
            </span>
        </header>
        <section class="content-wrapper">
            <h1 class="section-title content-width">Covarianza</h1>
            <article class="content-container content-width">
                <div class="definition environment" id="def2-26"><h2 class="environment-title">Definizione - Covarianza di due variabili casuali - Prima definizione</h2><div class="environment-body">     Considerando due variabili casuali <span class="math-span">\( X\)</span> e <span class="math-span">\( Y\)</span> per cui esiste valore atteso, si definisce la covarianza di <span class="math-span">\( X\)</span> e <span class="math-span">\( Y\)</span> come     <span class="math-block">\[         Cov(X, Y) = E\left[ \left( X - E[X] \right) \cdot \left( Y - E[Y] \right)  \right]         \]</span><span class="inner-title">Proprietà</span>     Considerando tale funzione si ha che:     <ul class="list-container"><li class="list-item">se <span class="math-span">\( X\)</span> "cresce" e <span class="math-span">\( Y\)</span> "decresce" (o viceversa) si ha che la covarianza è negativa, ovvero         <span class="math-block">\[             Cov(X, Y) \lt 0           \]</span></li><li class="list-item">se <span class="math-span">\( X\)</span> e <span class="math-span">\( Y\)</span> "crescono" (o "decrescono") entrambe si ha che la covarianza è positiva, ovvero         <span class="math-block">\[             Cov(X, Y) \gt 0         \]</span></li><li class="list-item">la covarianza è una funzione commutativa, ovvero         <span class="math-block">\[             Cov(X, Y) = Cov(Y, X)           \]</span></li><li class="list-item">la covarianza tra la stessa variabile è uguale alla varianza stessa, ovvero         <span class="math-block">\[             Cov(X, X) = Var(X)           \]</span></li><li class="list-item">considerando <span class="math-span">\( \alpha \in \mathbb{R}\)</span>, si ha che         <span class="math-block">\[             Cov(\alpha \cdot X, Y) = Cov(X, \alpha \cdot Y) = \alpha \cdot Cov(X, Y)         \]</span></li></ul></div></div><div class="demonstration environment" id="dem2-5"><h2 class="environment-title">Dimostrazione - Varianza della somma di due variabili casuali</h2><div class="environment-body">     Data la proposizione     <div class="proposition environment"><h3 class="environment-title">Enunciato</h3><div class="environment-body">         Considerando due variabili casuali <span class="math-span">\( X\)</span> e <span class="math-span">\( Y\)</span> per cui il valore atteso esiste, si ha che la varianza della somma di tali variabili è uguale a         <span class="math-block">\[             Var(X + Y) = Var(X) + Var(Y) + 2 \cdot Cov(X, Y)           \]</span></div></div><div class="proof environment"><h3 class="environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="toggle_proof(event)">visibility_off</span></h3><div class="environment-body hyde">         Per dimostrare tale proposizione, consideriamo che         <span class="math-block">\[             Z = X + Y         \]</span>         per cui si ha che         <span class="math-block">\[             E[Z] = E[X] + E[Y]           \]</span>         Calcolando quindi la varianza di <span class="math-span">\( Z\)</span> si ha che         <span class="math-block">\begin{aligned}             &amp; Var(Z) = E\left[(Z - E[Z])^2 \right] &amp; \iff         \end{aligned}</span>         e sostituendo a <span class="math-span">\( Z\)</span> si ottiene          <span class="math-block">\begin{aligned}             &amp; Var(X + Y) = E\left[(X + Y - E[X] - E[Y])^2 \right] &amp; \iff         \end{aligned}</span>         e per la proprietà associativa della somma si ha che         <span class="math-block">\begin{aligned}             &amp; Var(X + Y) = E\left[ \left( \left(X - E[X] \right) + \left( Y - E[Y] \right) \right)^2 \right] &amp; \iff         \end{aligned}</span>         Considerando ora il quadrato del binomio si ottiene         <span class="math-block">\begin{aligned}             &amp; Var(X + Y) = E\left[ \left(X - E[X] \right)^2 + \left( Y - E[Y] \right)^2 + 2 \cdot \left(X - E[X] \right) + \left( Y - E[Y] \right)  \right] &amp; \iff         \end{aligned}</span>         che per la linearità del valore atteso è uguale a         <span class="math-block">\begin{aligned}             &amp; Var(X + Y) = \underbrace{E\left[ \left(X - E[X] \right)^2 \right]}_{Var(X)} + \underbrace{E\left[ \left( Y - E[Y] \right)^2 \right]}_{Var(Y)} + 2 \cdot \underbrace{E \left[ \left(X - E[X] \right) + \left( Y - E[Y] \right)  \right]}_{Cov(X, Y)} &amp; \iff \\             &amp; Var(X + Y) = Var(X) + Var(Y) + 2 \cdot Cov(X, Y) &amp; \iff         \end{aligned}</span>         che dimostra la proposizione.     </div></div></div></div><div class="demonstration environment" id="dem2-6"><h2 class="environment-title">Dimostrazione - Covarianza di due variabili casuali - Seconda definizione</h2><div class="environment-body">     Data la proposizione     <div class="proposition environment"><h3 class="environment-title">Enunciato</h3><div class="environment-body">         Considerando due variabili casuali <span class="math-span">\( X\)</span> e <span class="math-span">\( Y\)</span> per cui esiste valore atteso, si ha che         <span class="math-block">\[             Cov(X, Y) = E[X \cdot Y] - E[X] \cdot E[Y]           \]</span></div></div><div class="proof environment"><h3 class="environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="toggle_proof(event)">visibility_off</span></h3><div class="environment-body hyde">         Per dimostrare tale proposizione consideriamo la definizione di covarianza e ridenominiamo <span class="math-span">\( E[X] = a\)</span> e <span class="math-span">\( E[Y] = b\)</span><span class="math-block">\begin{aligned}             &amp; Cov(X, Y) = E \left[ (X - a) \cdot (Y - b) \right] &amp; \iff         \end{aligned}</span>         Calcolando ora il prodotto interno         <span class="math-block">\begin{aligned}             &amp; Cov(X, Y) = E \left[ X \cdot Y - X \cdot b - a \cdot Y + a \cdot b  \right] &amp; \iff \\             &amp; Cov(X, Y) = E \left[ (X \cdot Y) - (X \cdot b) - (a \cdot Y) + (a \cdot b) \right] &amp; \iff         \end{aligned}</span>         e considerando la linearità del valore atteso, si ha         <span class="math-block">\begin{aligned}             &amp; Cov(X, Y) = E \left[ X \cdot Y \right] - E\left[X \cdot b \right] - E\left[ a \cdot Y \right] + E\left[ a \cdot b \right] &amp; \iff \\             &amp; Cov(X, Y) = E \left[ (X \cdot Y) \right] - b \cdot \underbrace{E[X]}_a - a \cdot \underbrace{E[Y]}_b + (a \cdot b) &amp; \iff \\             &amp; Cov(X, Y) = E \left[ (X \cdot Y) \right] - b \cdot a - b \cdot a + b \cdot a &amp; \iff \\             &amp; Cov(X, Y) = E \left[ (X \cdot Y) \right] - a \cdot b &amp; \iff         \end{aligned}</span>         e sostituendo ora <span class="math-span">\( a = E[X]\)</span> e <span class="math-span">\( b = E[Y]\)</span> si ottiene          <span class="math-block">\[             Cov(X, Y) = E \left[ (X \cdot Y) \right] - E[X] \cdot E[Y]         \]</span>         che dimostra la proposizione.     </div></div></div></div><div class="definition environment" id="def2-27"><h2 class="environment-title">Definizione - Coefficiente di correlazione</h2><div class="environment-body">     Considerando due variabili casuali <span class="math-span">\( X\)</span> e <span class="math-span">\( Y\)</span> per cui esiste valore atteso, si definisce il coefficente di correlazione di <span class="math-span">\( X\)</span> e <span class="math-span">\( Y\)</span> come     <span class="math-block">\[         Corr(X, Y) = \frac{Cov(X, Y)}{\sqrt{Var(X) \cdot Var(Y)}}       \]</span>     il cui valore è sempre compreso tra <span class="math-span">\( [-1, 1]\)</span>. </div></div><div class="demonstration environment" id="dem2-7"><h2 class="environment-title">Dimostrazione - Valore atteso e covarianza di variabili indipendenti</h2><div class="environment-body">     Dato il teorema     <div class="proposition environment"><h3 class="environment-title">Enunciato</h3><div class="environment-body">         Considerando due variabili casuali <span class="math-span">\( X\)</span> e <span class="math-span">\( Y\)</span> indipendenti per cui esiste valore atteso, si ha che         <span class="math-block">\[             E[X \cdot Y] = E[X] \cdot E[Y]           \]</span>         che implica che la covarianza sia uguale a <span class="math-span">\( 0\)</span>, ovvero         <span class="math-block">\[             Cov(X, Y) = 0           \]</span></div></div><div class="proof environment"><h3 class="environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="toggle_proof(event)">visibility_off</span></h3><div class="environment-body hyde"><span class="inner-title">Caso di variabili congiuntamente continue</span>         Per dimostrare questo teorema consideriamo da definizione che          <span class="math-block">\begin{aligned}             &amp; E[X \cdot Y] = \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} (x \cdot y) \cdot f(x, y) \ dx \ dy &amp; \iff          \end{aligned}</span>         Ora, considerando che <span class="math-span">\( X\)</span> e <span class="math-span">\( Y\)</span> sono indipendenti, si ha che la funzione di densità congiunta è uguale al prodotto delle funzioni di densità marginale, ovvero         <span class="math-block">\begin{aligned}             &amp; E[X \cdot Y] = \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} (x \cdot y) \cdot f_X(x) \cdot f_Y(y) \ dx \ dy &amp; \iff          \end{aligned}</span>         È ora possibile dividere i due integrali (in quanto ogni coppia è in funzione di una sola variabile) e sostituire i due integrali con la definizione di valore atteso, ovvero         <span class="math-block">\begin{aligned}             &amp; E[X \cdot Y] = \overbrace{\int_{-\infty}^{+\infty} x \cdot f_X(x) \ dx}^{E[X]} \ \cdot \overbrace{\int_{-\infty}^{+\infty} y \cdot f_Y(y) \ dy}^{E[Y]} &amp; \iff \\             &amp; E[X \cdot Y] = E[X] \cdot E[Y] &amp;         \end{aligned}</span>         che dimostra il teorema.         <br/>         Da ciò è possibile inoltre dimostrare che la covarianza è nulla, in quanto si ha che          <span class="math-block">\[             \begin{array}{lcl}                 Cov(X, Y) &amp; = &amp; \overbrace{E[X \cdot Y]}^{E[X] \cdot E[Y]} - E[X] \cdot E[Y] \\                 &amp; = &amp; 0             \end{array}         \]</span>         che dimostra l'implicazione.     </div></div><div class="mynote environment"><h3 class="environment-title">Osservazioni personali - Implicazione</h3><div class="environment-body">         Ciò significa che tutte le variabili indipendenti hanno covarianza (e quindi coefficiente di correlazione) nulla, ma non è vero il contrario, infatti non tutte le variabili con covarianza nulla sono indipendenti.     </div></div></div></div><div class="myexample environment" id="example25"><h2 class="environment-title">Esempio - Determinare funzioni di massa, valore atteso, varianza e covarianza di v. c. di Bernoulli</h2><div class="environment-body collapsed">     Considerando le variabili casuali di Bernoulli <span class="math-span">\( X \sim Be\left(\frac{3}{4}\right)\)</span> e <span class="math-span">\( Y \sim Be\left(\frac{3}{4}\right)\)</span> calcolare:     <ul class="list-container"><li class="list-item">le funzioni di massa congiunta e marginali;         </li><li class="list-item">il valore atteso <span class="math-span">\( E[X]\)</span> e <span class="math-span">\( E[Y]\)</span>;         </li><li class="list-item">la varianza di <span class="math-span">\( X\)</span> e di <span class="math-span">\( Y\)</span>;         </li><li class="list-item">la covarianza di <span class="math-span">\( X, Y\)</span>;         </li><li class="list-item">il coefficiente di correlazione;         </li><li class="list-item">se sono indipendenti.     </li></ul><span class="inner-title">Funzioni di massa di probabilità</span>     Per calcolare la funzione di massa è necessario considerare che sono variabili casuali di Bernoulli di parametro <span class="math-span">\( \frac{3}{4}\)</span>, ovvero si ha che conosciamo le funzioni di massa marginale, ovvero     <ul class="list-container"><li class="list-item"><span class="math-span">\( p_X(1) = \frac{3}{4}\)</span> e <span class="math-span">\( p_X(0) = 1 - \frac{3}{4} = \frac{1}{4}\)</span>;         </li><li class="list-item"><span class="math-span">\( p_Y(1) = \frac{3}{4}\)</span> e <span class="math-span">\( p_Y(0) = 1 - \frac{3}{4} = \frac{1}{4}\)</span>.     </li></ul>     Analogamente, per calcolare la funzione di massa congiunta è necessario considerare che:     <ul class="list-container"><li class="list-item"><span class="math-span">\( p(X = 1 \ \wedge \ Y = 0)\)</span> è uguale a <span class="math-span">\( 0\)</span>, in quanto è impossibile che coesistano i due eventi;         </li><li class="list-item"><span class="math-span">\( p(X = 0 \ \wedge \ Y = 1)\)</span> è uguale a <span class="math-span">\( 0\)</span>, in quanto è impossibile che coesistano i due eventi;         </li><li class="list-item"><span class="math-span">\( p(X = 0 \ \wedge \ Y = 0)\)</span> è uguale a <span class="math-span">\( p_X(0) - p(0, 1) = \frac{1}{4} - 0\)</span>, ovvero è uguale a <span class="math-span">\( \frac{1}{4}\)</span>;         </li><li class="list-item"><span class="math-span">\( p(X = 1 \ \wedge \ Y = 1)\)</span> è uguale a <span class="math-span">\( p_X(1) - p(1, 0) = \frac{3}{4} - 0\)</span>, ovvero è uguale a <span class="math-span">\( \frac{3}{4}\)</span>.     </li></ul><div class="image-environment"><div class="image-wrapper spaced-60"><img alt="Immagine" src="../resources/generated-18.png"/></div></div><span class="inner-title">Valore atteso <span class="math-span">\( E[X]\)</span> e <span class="math-span">\( E[Y]\)</span></span>     Dato che il valore atteso di una variabile di Bernoulli di parametro <span class="math-span">\( p\)</span> è proprio <span class="math-span">\( p\)</span>, si ha che     <span class="math-block">\[         E[X] = \frac{3}{4}         \]</span>     e     <span class="math-block">\[         E[Y] = \frac{3}{4}       \]</span><span class="inner-title">Varianza di <span class="math-span">\( X\)</span> e di <span class="math-span">\( Y\)</span></span>     Per calcolare la varianza si ha che è necessario calcolare <span class="math-span">\( E[X^2]\)</span>, ovvero     <span class="math-block">\[         \begin{array}{lcl}             E\left[X^2\right] = E\left[Y^2\right]  &amp; = &amp; 0^2 \cdot \frac{1}{4} + 1^2 \frac{3}{4} \\             &amp; = &amp; \frac{3}{4}         \end{array}       \]</span>     da cui      <span class="math-block">\[         \begin{array}{lcl}             Var(X) = Var(Y) &amp; = &amp; E\left[X^2\right] - \left( E[X] \right)^2 \\             &amp; = &amp; \frac{3}{4} - \frac{9}{16} \\             &amp; = &amp; \frac{3}{16}         \end{array}       \]</span><span class="inner-title">Covarianza di <span class="math-span">\( X\)</span> e <span class="math-span">\( Y\)</span></span>     Per calcolare la covarianza delle due variabili, si ha che è uguale a      <span class="math-block">\[         Cov(X, Y) = E[X \cdot Y] - E[X] \cdot E[Y]       \]</span>     È quindi necessario calcolare <span class="math-span">\( E[X \cdot Y]\)</span> che è uguale a     <span class="math-block">\[         \begin{array}{lcl}             E[X \cdot Y] &amp; = &amp; x_1 \cdot y_1 \cdot p(x_1, y_1) + x_1 \cdot y_2 \cdot p(x_1, y_2) + x_2 \cdot y_1 \cdot p(x_2, y_1) + x_2 \cdot y_2 \cdot p(x_2, y_2) \\             &amp; = &amp; 0 \cdot 0 \cdot 0 \cdot p(0,0) + 0 \cdot 1 \cdot p(0,1) + 1 \cdot 0 \cdot p(1,0) + 1 \cdot 1 \cdot p(1, 1) \\             &amp; = &amp; \frac{3}{4}         \end{array}     \]</span>     da cui     <span class="math-block">\[         \begin{array}{lcl}             Cov(X, Y) &amp; = &amp;  E[X \cdot Y] - E[X] \cdot E[Y] \\             &amp; = &amp; \frac{3}{4} - \frac{3}{4} \cdot \frac{3}{4}\\             &amp; = &amp; \frac{3}{4} - \frac{9}{16} \\             &amp; = &amp; \frac{3}{16}         \end{array}     \]</span><span class="inner-title">Coefficiente di correlazione</span>     Per calcolare il coefficiente di correlazione è necessario calcolare     <span class="math-block">\[         \begin{array}{lcl}             Corr(X, Y) &amp; = &amp; \frac{Cov(X, Y)}{\sqrt{Var(X) \cdot Var(Y)}} \\             &amp; = &amp; \frac{\frac{3}{16}}{\sqrt{\frac{3}{16} \cdot \frac{3}{16}}} \\             &amp; = &amp; 1         \end{array}       \]</span><span class="inner-title">Indipendenza</span>     Per verificare l' indipendenza è sufficiente verificare che la covarianza sia uguale a <span class="math-span">\( 0\)</span>: in questo caso non lo è quindi si ha che <span class="math-span">\( X\)</span> e <span class="math-span">\( Y\)</span> sono dipendenti. </div><div class="environment-tail"><span class="material-symbols-outlined body-visibility-icon" onclick="expand_environment(event)">expand_more</span></div></div>
            </article>
            <nav class="buttons-container content-width">
                <a class="navigation-button previous" href="varianza.html" rel="nofollow"><span>Varianza</span></a>
                <a class="navigation-button next" href="funzione-generatrice-di-momenti.html" rel="nofollow"><span>Funzione generatrice di momenti</span></a>
            </nav>
        </section>
        <div class="scroll-to-bottom-button" onclick="scroll_to_bottom()">
            <span class="material-symbols-outlined">
                keyboard_double_arrow_down
            </span>
        </div>
        <footer class="footer-wrapper">
            <div class="copyright-wrapper">
                <span> &copy; Copyright 2023</span> /
                <span>made by lorenzoarlo</span>
            </div>
            /
            <div class="privacy-wrapper">
                <span><a href="https://lorenzoarlo.github.io/privacy-and-cookies/" rel="nofollow">Pannello preferenze cookie</a></span> /
                <span><a href="https://lorenzoarlo.github.io/privacy-and-cookies/privacy-policy.html" rel="nofollow" target="_blank" >Privacy Policy</a></span>
            </div>
        </footer>
    </div>
</body>
</html>