<!DOCTYPE html>
<html lang="IT">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="stylesheet" href="../styles/style.css" />
    <link rel="stylesheet" href="../styles/index-style.css" />
    <link rel="stylesheet" href="../styles/content-style.css" />
    <link rel="icon" type="image/x-icon" href="../resources/favicon.ico">
    <meta name="application-name" content="Matematica applicata" />
    <meta name="apple-mobile-web-app-title" content="Matematica applicata" />
    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="#E34234" />
    <link rel="apple-touch-icon" href="../resources/favicon.png" />
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-78NHLXDQD8"></script>
    <script src="../scripts/script.js"></script>
    <style>:root { --bg-clr: #E34234; --fg-clr: #262626; }</style>
    <meta name="theme-color" content="#E34234" />
    <meta name="keywords" content="matematica applicata" />
    <meta name="description" content="Il seguente sito contiene gli appunti e le definizioni del corso 'Matematica applicata'.">
    <meta name="robots" content="index">
    <script defer async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <title>Matematica applicata - Variabili casuali - Modelli di variabili casuali discrete</title>
</head>
<body>
    <div class="container">
        <header class="header-wrapper">
            <div class="title-wrapper">
                <span class="title">
                    Matematica applicata
                </span>
                <span class="subtitle">
                    by lorenzoarlo
                </span>
            </div>
            <span class="page-title">
                Variabili casuali
            </span>
        </header>
        <section class="content-wrapper">
            <h1 class="section-title content-width">Modelli di variabili casuali discrete</h1>
            <article class="content-container content-width">
                <div class="definition environment" id="def2-28"><h2 class="environment-title">Definizione - Variabile casuale di Bernoulli (o Bernoulliane)</h2><div class="environment-body">     Una variabile casuale discreta <span class="math-span">\( X\)</span> si definisce di Bernoulli di parametro <span class="math-span">\( p\)</span> (<span class="math-span">\( X \sim Be(p)\)</span>) se è caratterizzata da due soli valori (detti di "successo" o "fallimento"), ovvero     <span class="math-block">\[         X \in \{ 0, 1 \}       \]</span>     dove      <span class="math-block">\[         P(X = 1) = p     \]</span>     e che      <span class="math-block">\[         P(X = 0) = q = 1 - p       \]</span>     La variabile casuale si presenterà quindi come     <span class="math-block">\[         X = \left\{              \begin{array}{ll}                 1 &amp; \text{se si verifica l'evento} \ A \\                 0 &amp; \text{se non si verifica l'evento} \ A              \end{array}         \right.     \]</span><span class="inner-title">Valore atteso</span>     Il valore atteso di una v. c. di Bernoulli di parametro <span class="math-span">\( p\)</span>, calcolabile come     <span class="math-block">\[         \begin{array}{ccl}             E[X] &amp; = &amp; 0 \cdot q + 1 \cdot p \\             &amp; = &amp; p         \end{array}     \]</span>     è uguale a <span class="math-span">\( p\)</span>.     <span class="inner-title">Momento di ordine <span class="math-span">\( n\)</span></span>     Il momento di ordine <span class="math-span">\( n\)</span> di una v. c. di Bernoulli di parametro <span class="math-span">\( p\)</span>, calcolabile come     <span class="math-block">\[         \begin{array}{ccl}             E[X^n] &amp; = &amp; 0^n \cdot q + 1^n \cdot p \\             &amp; = &amp; p         \end{array}     \]</span>     è uguale a <span class="math-span">\( p\)</span>, per ogni <span class="math-span">\( n \in \mathbb{N}\)</span>.     <span class="inner-title">Varianza</span>     La varianza di una v. c. di Bernoulli di parametro <span class="math-span">\( p\)</span>, calcolabile come     <span class="math-block">\[         \begin{array}{ccl}             Var(X) &amp; = &amp; E[X^2] - \left( E[X] \right)^2 \\             &amp; = &amp; p - p^2 \\             &amp; = &amp; p \cdot (1 - p) \\             &amp; = &amp; p \cdot q         \end{array}     \]</span>     è uguale a <span class="math-span">\( p \cdot q\)</span>.     <span class="inner-title">Funzione generatrice dei momenti</span>     La funzione generatrice dei momenti di una v. c. di Bernoulli di parametro <span class="math-span">\( p\)</span>, calcolabile come     <span class="math-block">\[         \begin{array}{ccl}             \phi(t) &amp; = &amp; E\left[\mathrm{e}^{t \cdot X}\right] \\             &amp; = &amp; \sum_{k = 1}^2 \mathrm{e}^{t \cdot x_k} \cdot p(x_k) \\             &amp; = &amp; \mathrm{e}^{t \cdot 0} \cdot q + \mathrm{e}^{t \cdot 1} \cdot p \\             &amp; = &amp; q + \mathrm{e}^t \cdot p         \end{array}     \]</span>     è uguale a <span class="math-span">\( q + \mathrm{e}^t \cdot p\)</span>. </div></div><div class="myexample environment" id="example26"><h2 class="environment-title">Esempio - Determinare funzioni di massa e covarianza per una coppia di v. c. discrete</h2><div class="environment-body collapsed">     Considerando una variabile casuale di Bernoulli <span class="math-span">\( X\)</span> di parametro <span class="math-span">\( \frac{1}{2}\)</span> (ovvero, <span class="math-span">\( X \sim Be\left(\frac{1}{2}\right)\)</span>) e <span class="math-span">\( Y\)</span> una variabile casuale discreta tale che      <span class="math-block">\[         P(Y = 1 \ \mid \ X = 0) = \varepsilon         \]</span>     ovvero ci è data la che <span class="math-span">\( Y\)</span> sia <span class="math-span">\( 1\)</span> dato <span class="math-span">\( X\)</span> uguale a <span class="math-span">\( 0\)</span> (ovvero vi è probabilità condizionata) e che     <span class="math-block">\[         P(Y = 0 \ \mid \ X = 1) = 2 \cdot \varepsilon             \]</span>     con <span class="math-span">\( \varepsilon \in ] 0, \frac{1}{2} [\)</span>, determinare:     <ul class="list-container"><li class="list-item">la funzione di massa congiunta e le funzioni di massa marginale;         </li><li class="list-item">la covarianza di <span class="math-span">\( X\)</span> e <span class="math-span">\( Y\)</span>;         </li><li class="list-item">i valori di <span class="math-span">\( \varepsilon\)</span> (se esistono) per cui <span class="math-span">\( X\)</span> e <span class="math-span">\( Y\)</span> sono scorrelate (ovvero <span class="math-span">\( Cov(X, Y) = 0\)</span>)     </li></ul><span class="inner-title">Funzione di massa congiunta e funzioni di massa marginale</span>     Considerando tali variabili, si ha che conosciamo già la funzione di massa marginale <span class="math-span">\( p_X\)</span> (dato che è una variabile di Bernoulli), in quanto sappiamo che     <span class="math-block">\[         p_X(1) = \frac{1}{2}     \]</span>     e che      <span class="math-block">\[         p_X(0) = 1 - \frac{1}{2} = \frac{1}{2}     \]</span>     e, unendo ciò alle informazioni date dalla consegna, è possibile calcolare     <ul class="list-container"><li class="list-item">il valore di <span class="math-span">\( p(1, 0)\)</span> che è uguale a         <span class="math-block">\[             \begin{array}{ccl}                 P(X = 1 \ \wedge \ Y = 0) &amp; = &amp; P(Y = 0 \ \mid \ X = 1) \cdot p_X(1) \\                 &amp; = &amp; 2 \cdot \varepsilon \cdot \frac{1}{2} \\                 &amp; = &amp; \varepsilon             \end{array}         \]</span></li><li class="list-item">il valore di <span class="math-span">\( p(0, 1)\)</span> che è uguale a         <span class="math-block">\[             \begin{array}{ccl}                 P(X = 0 \ \wedge \ Y = 1) &amp; = &amp; P(Y = 1 \ \mid \ X = 0) \cdot p_X(0) \\                 &amp; = &amp; \varepsilon \cdot \frac{1}{2} \\                 &amp; = &amp; \frac{\varepsilon}{2}             \end{array}         \]</span></li></ul>     da cui è infine possibile calcolare anche i rimanenti valori:     <ul class="list-container"><li class="list-item">il valore di <span class="math-span">\( p(0, 0)\)</span> che è uguale a         <span class="math-block">\[             \begin{array}{ccl}                 P(X = 0 \ \wedge \ Y = 0) &amp; = &amp;  p_X(0) - p(0, 1) \\                 &amp; = &amp; \frac{1}{2} - \frac{\varepsilon}{2}             \end{array}         \]</span></li><li class="list-item">il valore di <span class="math-span">\( p(1, 1)\)</span> che è uguale a         <span class="math-block">\[             \begin{array}{ccl}                 P(X = 1 \ \wedge \ Y = 1) &amp; = &amp;  p_X(1) - p(1, 0) \\                 &amp; = &amp; \frac{1}{2} - \varepsilon             \end{array}         \]</span></li></ul>     Da ciò è anche possibile ottenere la funzione <span class="math-span">\( p_Y\)</span> come     <ul class="list-container"><li class="list-item"><span class="math-span">\( p_Y(0)\)</span> è uguale a         <span class="math-block">\[             \begin{array}{ccl}                 p_Y(0) &amp; = &amp; p(0, 0) + p(1, 0) \\                 &amp; = &amp; \frac{1}{2} - \frac{\varepsilon}{2} + \varepsilon \\                 &amp; = &amp; \frac{1}{2} + \frac{\varepsilon}{2}             \end{array}         \]</span></li><li class="list-item"><span class="math-span">\( p_Y(1)\)</span> è uguale a         <span class="math-block">\[             \begin{array}{ccl}                 p_Y(1) &amp; = &amp; p(0, 1) + p(1, 1) \\                 &amp; = &amp; \frac{\varepsilon}{2} + \frac{1}{2} - \varepsilon \\                 &amp; = &amp; \frac{1}{2} - \frac{\varepsilon}{2}             \end{array}         \]</span></li></ul>     È infine possibile ottenere     <div class="image-environment"><div class="image-wrapper spaced-60"><img alt="Immagine" src="../resources/generated-18.png"/></div></div><span class="inner-title">Covarianza</span>     Per calcolare la covarianza di <span class="math-span">\( X\)</span> e <span class="math-span">\( Y\)</span>, si ha che è necessario calcolare     <span class="math-block">\[         Cov(X, Y) = E[X \cdot Y] - E[X] \cdot E[Y]      \]</span>     Si ha quindi che:     <ul class="list-container"><li class="list-item">il valore atteso <span class="math-span">\( E[X]\)</span> è uguale a <span class="math-span">\( \frac{1}{2}\)</span> in quanto variabile di Bernoulli di parametro <span class="math-span">\( \frac{1}{2}\)</span>;         </li><li class="list-item">il valori atteso <span class="math-span">\( E[Y]\)</span> è uguale         <span class="math-block">\[             \begin{array}{ccl}                 E[Y] &amp; = &amp; 0 \cdot p_Y(0) + 1 \cdot p_Y(1) \\                 &amp; = &amp; \frac{1}{2} - \frac{\varepsilon}{2}             \end{array}         \]</span></li><li class="list-item">il valore atteso <span class="math-span">\( E[X \cdot Y]\)</span> è necessario calcolarlo come         <span class="math-block">\[             \begin{array}{ccl}                 E[X \cdot Y] &amp; = &amp; \sum_{i = 1}^{2} \sum_{j = 1}^{2} x_i \cdot y_j \cdot p(x_i, y_j) \\                 &amp; = &amp; 0 \cdot 0 \cdot p(0, 0) + 0 \cdot 1 \cdot p(0, 1) + 1 \cdot 0 \cdot p(1,0) + 1 \cdot 1 \cdot p(1, 1) \\                 &amp; = &amp; \frac{1}{2} - \varepsilon             \end{array}         \]</span>         dato che non sono indipendenti tra loro generalmente (come è evidente dalla funzione di massa).     </li></ul>     Unendo ciò, si ha che      <span class="math-block">\[         \begin{array}{ccl}             Cov(X, Y) &amp; = &amp; E[X \cdot Y] - E[X] \cdot E[Y] \\             &amp; = &amp; \frac{1}{2} - \varepsilon - \frac{1}{2} \cdot \left( \frac{1}{2} - \frac{\varepsilon}{2} \right) \\             &amp; = &amp; \frac{1}{2} - \varepsilon - \frac{1}{4} + \frac{\varepsilon}{4} \\             &amp; = &amp; \frac{1}{4} - \frac{3}{4} \cdot \varepsilon         \end{array}       \]</span><span class="inner-title"><span class="math-span">\( \varepsilon\)</span> per cui <span class="math-span">\( X\)</span> e <span class="math-span">\( Y\)</span> sono scorrelate</span>     Per calcolare i valori di <span class="math-span">\( \varepsilon\)</span> per cui <span class="math-span">\( X\)</span> e <span class="math-span">\( Y\)</span> sono scorrelate, si ha che è sufficiente risolvere la seguente equazione in <span class="math-span">\( \varepsilon\)</span>:     <span class="math-block">\begin{aligned}         &amp; \frac{1}{4} - \frac{3}{4} \cdot \varepsilon = 0 &amp; \iff \\         &amp; \frac{1}{4} = \frac{3}{4} \cdot \varepsilon &amp; \iff \\         &amp; \frac{1}{3} = \varepsilon &amp;     \end{aligned}</span>     Dato che <span class="math-span">\( \varepsilon \in ] 0, \frac{1}{2} [\)</span>, si ha che <span class="math-span">\( \varepsilon = \frac{1}{3}\)</span> è valido, rendendo quindi le due variabili scorrelate per tale valore. </div><div class="environment-tail"><span class="material-symbols-outlined body-visibility-icon" onclick="expand_environment(event)">expand_more</span></div></div><div class="myexample environment" id="example27"><h2 class="environment-title">Esempio - Determinare funzioni di massa, valore atteso, varianza e covarianza di v. c. di Bernoulli</h2><div class="environment-body collapsed">     Considerando le variabili casuali di Bernoulli <span class="math-span">\( X \sim Be\left(\frac{3}{4}\right)\)</span> e <span class="math-span">\( Y \sim Be\left(\frac{3}{4}\right)\)</span> calcolare:     <ul class="list-container"><li class="list-item">le funzioni di massa congiunta e marginali;         </li><li class="list-item">il valore atteso <span class="math-span">\( E[X]\)</span> e <span class="math-span">\( E[Y]\)</span>;         </li><li class="list-item">la varianza di <span class="math-span">\( X\)</span> e di <span class="math-span">\( Y\)</span>;         </li><li class="list-item">la covarianza di <span class="math-span">\( X, Y\)</span>;         </li><li class="list-item">il coefficiente di correlazione;         </li><li class="list-item">se sono indipendenti.     </li></ul><span class="inner-title">Funzioni di massa di probabilità</span>     Per calcolare la funzione di massa è necessario considerare che sono variabili casuali di Bernoulli di parametro <span class="math-span">\( \frac{3}{4}\)</span>, ovvero si ha che conosciamo le funzioni di massa marginale, ovvero     <ul class="list-container"><li class="list-item"><span class="math-span">\( p_X(1) = \frac{3}{4}\)</span> e <span class="math-span">\( p_X(0) = 1 - \frac{3}{4} = \frac{1}{4}\)</span>;         </li><li class="list-item"><span class="math-span">\( p_Y(1) = \frac{3}{4}\)</span> e <span class="math-span">\( p_Y(0) = 1 - \frac{3}{4} = \frac{1}{4}\)</span>.     </li></ul>     Analogamente, per calcolare la funzione di massa congiunta è necessario considerare che:     <ul class="list-container"><li class="list-item"><span class="math-span">\( P(X = 1 \ \wedge \ Y = 0)\)</span> è uguale a <span class="math-span">\( 0\)</span>, in quanto è impossibile che coesistano i due eventi;         </li><li class="list-item"><span class="math-span">\( P(X = 0 \ \wedge \ Y = 1)\)</span> è uguale a <span class="math-span">\( 0\)</span>, in quanto è impossibile che coesistano i due eventi;         </li><li class="list-item"><span class="math-span">\( P(X = 0 \ \wedge \ Y = 0)\)</span> è uguale a <span class="math-span">\( p_X(0) - p(0, 1) = \frac{1}{4} - 0\)</span>, ovvero è uguale a <span class="math-span">\( \frac{1}{4}\)</span>;         </li><li class="list-item"><span class="math-span">\( P(X = 1 \ \wedge \ Y = 1)\)</span> è uguale a <span class="math-span">\( p_X(1) - p(1, 0) = \frac{3}{4} - 0\)</span>, ovvero è uguale a <span class="math-span">\( \frac{3}{4}\)</span>.     </li></ul><div class="image-environment"><div class="image-wrapper spaced-60"><img alt="Immagine" src="../resources/generated-19.png"/></div></div><span class="inner-title">Valore atteso <span class="math-span">\( E[X]\)</span> e <span class="math-span">\( E[Y]\)</span></span>     Dato che il valore atteso di una variabile di Bernoulli di parametro <span class="math-span">\( p\)</span> è proprio <span class="math-span">\( p\)</span>, si ha che     <span class="math-block">\[         E[X] = \frac{3}{4}         \]</span>     e     <span class="math-block">\[         E[Y] = \frac{3}{4}       \]</span><span class="inner-title">Varianza di <span class="math-span">\( X\)</span> e di <span class="math-span">\( Y\)</span></span>     Per calcolare la varianza si ha che è necessario calcolare <span class="math-span">\( E[X^2]\)</span>, ovvero     <span class="math-block">\[         \begin{array}{lcl}             E\left[X^2\right] = E\left[Y^2\right]  &amp; = &amp; 0^2 \cdot \frac{1}{4} + 1^2 \frac{3}{4} \\             &amp; = &amp; \frac{3}{4}         \end{array}       \]</span>     da cui      <span class="math-block">\[         \begin{array}{lcl}             Var(X) = Var(Y) &amp; = &amp; E\left[X^2\right] - \left( E[X] \right)^2 \\             &amp; = &amp; \frac{3}{4} - \frac{9}{16} \\             &amp; = &amp; \frac{3}{16}         \end{array}       \]</span><span class="inner-title">Covarianza di <span class="math-span">\( X\)</span> e <span class="math-span">\( Y\)</span></span>     Per calcolare la covarianza delle due variabili, si ha che è uguale a      <span class="math-block">\[         Cov(X, Y) = E[X \cdot Y] - E[X] \cdot E[Y]       \]</span>     È quindi necessario calcolare <span class="math-span">\( E[X \cdot Y]\)</span> che è uguale a     <span class="math-block">\[         \begin{array}{lcl}             E[X \cdot Y] &amp; = &amp; x_1 \cdot y_1 \cdot p(x_1, y_1) + x_1 \cdot y_2 \cdot p(x_1, y_2) + x_2 \cdot y_1 \cdot p(x_2, y_1) + x_2 \cdot y_2 \cdot p(x_2, y_2) \\             &amp; = &amp; 0 \cdot 0 \cdot 0 \cdot p(0,0) + 0 \cdot 1 \cdot p(0,1) + 1 \cdot 0 \cdot p(1,0) + 1 \cdot 1 \cdot p(1, 1) \\             &amp; = &amp; \frac{3}{4}         \end{array}     \]</span>     da cui     <span class="math-block">\[         \begin{array}{lcl}             Cov(X, Y) &amp; = &amp;  E[X \cdot Y] - E[X] \cdot E[Y] \\             &amp; = &amp; \frac{3}{4} - \frac{3}{4} \cdot \frac{3}{4}\\             &amp; = &amp; \frac{3}{4} - \frac{9}{16} \\             &amp; = &amp; \frac{3}{16}         \end{array}     \]</span><span class="inner-title">Coefficiente di correlazione</span>     Per calcolare il coefficiente di correlazione è necessario calcolare     <span class="math-block">\[         \begin{array}{lcl}             Corr(X, Y) &amp; = &amp; \frac{Cov(X, Y)}{\sqrt{Var(X) \cdot Var(Y)}} \\             &amp; = &amp; \frac{\frac{3}{16}}{\sqrt{\frac{3}{16} \cdot \frac{3}{16}}} \\             &amp; = &amp; 1         \end{array}       \]</span><span class="inner-title">Indipendenza</span>     Per verificare l' indipendenza è sufficiente verificare che la covarianza sia uguale a <span class="math-span">\( 0\)</span>: in questo caso non lo è quindi si ha che <span class="math-span">\( X\)</span> e <span class="math-span">\( Y\)</span> sono dipendenti. </div><div class="environment-tail"><span class="material-symbols-outlined body-visibility-icon" onclick="expand_environment(event)">expand_more</span></div></div><div class="definition environment" id="def2-29"><h2 class="environment-title">Definizione - Variabile casuale binomiale</h2><div class="environment-body">     Una variabile casuale discreta <span class="math-span">\( X\)</span> si definisce binomiale di parametri <span class="math-span">\( n\)</span> e <span class="math-span">\( p\)</span> (<span class="math-span">\( X \sim B(n, p)\)</span>) se, considerando il caso in cui si ripete un esperimento <span class="math-span">\( n\)</span> volte in maniera identica e indipendente e la probabilità che un certo esito <span class="math-span">\( A\)</span> si verifichi sia <span class="math-span">\( p\)</span>, <span class="math-span">\( X\)</span> rappresenta il numero di esperimenti (sugli <span class="math-span">\( n\)</span> totali) in cui <span class="math-span">\( A\)</span> si è verificato.     <br/>     Si ha quindi che <span class="math-span">\( X\)</span> è caratterizzata da <span class="math-span">\( n + 1\)</span> valori, ovvero     <span class="math-block">\[         X \in \{ 0, 1, \ldots, n \}       \]</span><span class="inner-title"><span class="math-span">\( P(X = k)\)</span></span>     Considerando un valore <span class="math-span">\( k \in \{ 0, 1, \ldots, n \}\)</span>, calcolare <span class="math-span">\( P(X = k)\)</span> equivale a calcolare la probabilità che <span class="math-span">\( A\)</span> si verifichi <span class="math-span">\( k\)</span> volte, data dal prodotto tra:     <ul class="list-container"><li class="list-item">il numero totale di possibili combinazioni in cui si hanno <span class="math-span">\( k\)</span> successi, ovvero <span class="math-span">\( C_{n, k}\)</span>, ovvero <span class="math-span">\( \left( \begin{array}{c} n \\ k \end{array} \right)\)</span>;         </li><li class="list-item">la probabilità che <span class="math-span">\( A\)</span> si verifichi <span class="math-span">\( k\)</span> volte, ovvero <span class="math-span">\( p^k\)</span>;         </li><li class="list-item">la probabilità che <span class="math-span">\( A\)</span> non si verifichi <span class="math-span">\( n - k\)</span> volte, ovvero <span class="math-span">\( (1 - p)^{n - k} = q^{n - k}\)</span>;     </li></ul>      ovvero      <span class="math-block">\[         P(X = k) = \left( \begin{array}{c} n \\ k \end{array} \right)  \cdot p^k \cdot q^{n - k}     \]</span><span class="inner-title">Valore atteso</span>     Il valore atteso di una v. c. binomiale di parametri <span class="math-span">\( n\)</span> e <span class="math-span">\( k\)</span> si ha che sarebbe uguale a     <span class="math-block">\[         \begin{array}{ccl}             E[X] &amp; = &amp; \sum_{k = 0}^n k \cdot P(X = k) \\             &amp; = &amp; \sum_{k = 0}^n k \cdot C_{n, k} \cdot p^k \cdot q^{n - k}         \end{array}     \]</span>     che risulta essere un calcolo abbastanza ostico.     <br/>     È però possibile considerare che <span class="math-span">\( X\)</span> sia uguale alla somma di <span class="math-span">\( n\)</span> variabili casuali di Bernoulli che indicano se l'<span class="math-span">\( i\)</span>-esimo esperimento (degli <span class="math-span">\( n\)</span> totali) si è verificato, ovvero     <span class="math-block">\[         X = \sum_{i = 1}^n X_i \qquad \text{con} \ X_i \sim Be(p)       \]</span>     Considerato ciò, si ha che     <span class="math-block">\[         \begin{array}{ccl}             E[X] &amp; = &amp; E\left[\sum_{i = 1}^n X_i \right] \\             &amp; = &amp; \sum_{i = 1}^n E[X_i] \\             &amp; = &amp; n \cdot E[X_i] \\             &amp; = &amp; n \cdot p         \end{array}     \]</span><span class="inner-title">Varianza</span>     La varianza di una v. c. binomiale di parametri <span class="math-span">\( n\)</span> e <span class="math-span">\( k\)</span>, considerando che <span class="math-span">\( X\)</span> è somma di <span class="math-span">\( n\)</span> variabili casuali di Bernoulli e che ogni esperimento è svolto in maniera indipendente per definizione, si ha che è uguale a     <span class="math-block">\[         \begin{array}{ccl}             Var(X) &amp; = &amp; Var\left( \sum_{i = 1}^n X_i \right) \\             &amp; \underset{\text{indip.}}{=} &amp; \sum_{i = 1}^n Var(X_i) \\             &amp; = &amp; \sum_{i = 1}^n p \cdot q \\             &amp; = &amp; n \cdot p \cdot q         \end{array}     \]</span><span class="inner-title">Funzione generatrice dei momenti</span>     La funzione generatrice dei momenti di una v. c. binomiale di parametri <span class="math-span">\( n\)</span> e <span class="math-span">\( p\)</span>, considerando che <span class="math-span">\( X\)</span> è somma di <span class="math-span">\( n\)</span> variabili casuali di Bernoulli e che ogni esperimento è svolto in maniera indipendente per definizione, si ha che è uguale a     <span class="math-block">\[         \begin{array}{ccl}             \phi_X(t) &amp; = &amp; E\left[ \mathrm{e}^{t \cdot X} \right] \\             &amp; = &amp; E\left[ \mathrm{e}^{t \cdot \sum_{i = 1}^n X_i} \right] \\             &amp; = &amp; E\left[ \prod_{i = 1}^n \mathrm{e}^{t \cdot X_i} \right] \\             &amp; \underset{\text{indip.}}{=} &amp; \prod_{i = 1}^n E\left[ \mathrm{e}^{t \cdot X_i} \right] \\             &amp; = &amp; \prod_{i = 1}^n \phi_{X_i}(t) \\              &amp; = &amp; \prod_{i = 1}^n q + \mathrm{e}^{t} \cdot p \\             &amp; = &amp; \left( q + \mathrm{e}^{t} \cdot p  \right)^n         \end{array}     \]</span><div class="mynote environment"><h3 class="environment-title">Osservazioni personali - V. c. binomiale con <span class="math-span">\( n = 1\)</span> come v. c. di Bernoulli</h3><div class="environment-body">         Nel caso si abbia una variabile casuale binomiale di parametri <span class="math-span">\( 1\)</span> e <span class="math-span">\( p\)</span>, si ha che essa è una variabile casuale di Bernoulli di parametro <span class="math-span">\( p\)</span>, ovvero         <span class="math-block">\[             X \sim B(1, p) \sim Be(p)           \]</span></div></div></div></div><div class="demonstration environment" id="dem2-8"><h2 class="environment-title">Dimostrazione - Proprietà di riproducibilità di variabili casuali binomiali</h2><div class="environment-body">     Data la proposizione     <div class="proposition environment"><h3 class="environment-title">Enunciato</h3><div class="environment-body">         Considerando due variabili casuali binomiali indipendenti <span class="math-span">\( X\)</span> e <span class="math-span">\( Y\)</span>, tali che <span class="math-span">\( X \sim B(n, p)\)</span> e <span class="math-span">\( Y \sim B(m, p)\)</span> (ovvero con parametro <span class="math-span">\( p\)</span> uguale), si ha che         <span class="math-block">\[             X + Y \sim B(n + m, p)            \]</span></div></div><div class="proof environment"><h3 class="environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="toggle_proof(event)">visibility_off</span></h3><div class="environment-body hyde">         Per dimostrare questa proposizione, considerando le funzioni generatrici dei momenti, si dovrebbe avere che         <span class="math-block">\[             \phi_{X + Y}(t) = (q + \mathrm{e}^t \cdot p)^{n + m}           \]</span>         Considerando la definizione, si ha che         <span class="math-block">\begin{aligned}             &amp; \phi_{X + Y}(t) = E\left[ \mathrm{e}^{t \cdot (X + Y)} \right] &amp; \iff \\             &amp; \phi_{X + Y}(t) = E\left[ \mathrm{e}^{t \cdot X + t \cdot Y} \right] &amp; \iff \\             &amp; \phi_{X + Y}(t) = E\left[ \mathrm{e}^{t \cdot X} \cdot \mathrm{e}^{t \cdot Y} \right] &amp; \iff         \end{aligned}</span>         e per l'indipendenza delle variabili, si ha che         <span class="math-block">\begin{aligned}             &amp; \phi_{X + Y}(t) = E\left[ \mathrm{e}^{t \cdot X} \right] \cdot E\left[ \mathrm{e}^{t \cdot Y} \right] &amp; \iff \\             &amp; \phi_{X + Y}(t) = (q + \mathrm{e}^t \cdot p)^n \cdot (q + \mathrm{e}^t \cdot p)^m &amp; \iff \\             &amp; \phi_{X + Y}(t) = (q + \mathrm{e}^t \cdot p)^{n + m} &amp;         \end{aligned}</span>         che dimostra la proposizione.     </div></div></div></div><div class="definition environment" id="def2-30"><h2 class="environment-title">Definizione - Variabile casuale geometrica</h2><div class="environment-body">     Una variabile casuale discreta <span class="math-span">\( X\)</span> si definisce geometrica di parametro <span class="math-span">\( p\)</span> (<span class="math-span">\( X \sim G(p)\)</span>) se, considerando il caso in cui si ripete un esperimento in maniera identica e indipendente (per infinite volte) e la probabilità che un certo esito <span class="math-span">\( A\)</span> si verifichi sia <span class="math-span">\( p\)</span>, <span class="math-span">\( X\)</span> rappresenta il <strong>numero di esperimenti eseguiti affinchè <span class="math-span">\( A\)</span> si sia verificato</strong>.     <br/>     Si ha quindi che <span class="math-span">\( X\)</span> è caratterizzata da un numero infinito di valori, ovvero     <span class="math-block">\[         X \in \{ 1, \ldots, +\infty \}       \]</span><span class="inner-title"><span class="math-span">\( P(X = k)\)</span></span>     Considerando un valore <span class="math-span">\( k \in \{ 1, \ldots, +\infty \}\)</span>, calcolare <span class="math-span">\( P(X = k)\)</span> equivale a calcolare la probabilità che <span class="math-span">\( A\)</span> si verifichi al <span class="math-span">\( k\)</span>-esimo tentativo, ovvero la probabilità che <span class="math-span">\( A\)</span> non si verifichi per <span class="math-span">\( k - 1\)</span> volte (ovvero <span class="math-span">\( q^{k - 1}\)</span>) e che si verifichi una volta (ovvero <span class="math-span">\( p\)</span>), per cui     <span class="math-block">\[         P(X = k) = q^{k - 1} \cdot p     \]</span><span class="inner-title">Probabilità totali</span>     Considerando il valore di una serie geometrica     <span class="math-block">\[         \sum_{k = 1}^{+\infty} r^k = \frac{1}{1 - r} \qquad \text{con} \ r \in ]0, 1[       \]</span>     Si ha che      <span class="math-block">\[         \begin{array}{ccl}             \sum_{k = 1}^{+\infty} P(X = k) &amp; = &amp; \sum_{k = 1}^{+\infty} q^{k - 1} \cdot p \\             &amp; = &amp; p \cdot \sum_{k = 1}^{+\infty} q^{k - 1}         \end{array}     \]</span>     e, considerando <span class="math-span">\( j = k - 1\)</span>, si ha che <span class="math-span">\( k = 1 \implies j = 0\)</span>, per cui     <span class="math-block">\[         \begin{array}{ccl}             \sum_{k = 1}^{+\infty} p(X = k) &amp; = &amp; p \cdot \sum_{j = 0}^{+\infty} q^{j} \\             &amp; = &amp; p \cdot \sum_{j = 0}^{+\infty} q^{j} \\             &amp; = &amp; p \cdot \frac{1}{1 - q} \\             &amp; = &amp; \frac{p}{1 - q} \\             &amp; = &amp; \frac{p}{p} \\              &amp; = &amp; 1         \end{array}     \]</span><span class="inner-title">Valore atteso</span>     Il valore atteso di una v. c. geometrica di parametro <span class="math-span">\( p\)</span> si ha che sarebbe uguale a      <span class="math-block">\begin{aligned}         &amp; E[X] = \sum_{k = 1}^{+\infty} k \cdot p(k) &amp; =      \end{aligned}</span>     ovvero      <span class="math-block">\begin{aligned}         = \quad &amp; \sum_{k = 1}^{+\infty} k \cdot q^{k - 1} \cdot p &amp; = \\         = \quad &amp; p \cdot \sum_{k = 1}^{+\infty} k \cdot q^{k - 1} &amp; =     \end{aligned}</span>     ora, sapendo che <span class="math-span">\( k \cdot q^{k - 1}\)</span> è la derivata di <span class="math-span">\( q^k\)</span>, ovvero <span class="math-span">\( \frac{d}{dq} q^k\)</span>, si ha che     <span class="math-block">\begin{aligned}         = \quad &amp; p \cdot \sum_{k = 1}^{+\infty} \frac{d}{dq} q^k &amp; = \\         = \quad &amp; p \cdot  \frac{d}{dq} \left( \sum_{k = 1}^{+\infty} q^k \right) &amp; =      \end{aligned}</span>     Ora, dato che <span class="math-span">\( \sum_{k = 1}^{+\infty} q^k\)</span> è una serie geometrica, si ha che è uguale a <span class="math-span">\( \frac{1}{1 - q}\)</span>, ovvero     <span class="math-block">\begin{aligned}         = \quad &amp; p \cdot  \frac{d}{dq} \left( \frac{1}{1 - q} \right) &amp; = \\         = \quad &amp; p \cdot  \frac{d}{dq} \left( (1 - q)^{-1} \right) &amp; =  \\     \end{aligned}</span>     Considerando ora la derivata di <span class="math-span">\( (1 - q)^{-1}\)</span>, per le regole di derivazione composta, si ha che è uguale     <span class="math-block">\begin{aligned}         = \quad &amp; p \cdot (-1) \cdot \left( - (1 - q)^{-2} \right) &amp; = \\         = \quad &amp; p \cdot \frac{1}{(1 - q)^2} &amp; = \\         = \quad &amp; p \cdot \frac{1}{p^2} &amp; = \\         = \quad &amp; \frac{1}{p} &amp;         \end{aligned}</span>     Si ha quindi che <span class="math-span">\( E[X] = \frac{1}{p}\)</span>.      <span class="inner-title">Varianza</span>     La varianza di una v. c. geometrica è calcolabile come      <span class="math-block">\[         Var(X) = E\left[X^2\right] - \left( E[X] \right)^2      \]</span>     È necessario calcolare quindi <span class="math-span">\( E\left[ X^2 \right]\)</span>, che è uguale a     <span class="math-block">\begin{aligned}         &amp; E\left[X^2\right] =  \sum_{k = 1}^{+\infty} k^2 \cdot p(k) &amp; =     \end{aligned}</span>     ovvero     <span class="math-block">\begin{aligned}         = \quad &amp; \sum_{k = 1}^{+\infty} k^2 \cdot q^{k - 1} \cdot p &amp; = \\         = \quad &amp; \sum_{k = 1}^{+\infty} k^2 \cdot q \cdot q^{k - 2} \cdot p &amp; = \\         = \quad &amp; p \cdot q \cdot \sum_{k = 1}^{+\infty} k^2 \cdot q^{k - 2} &amp; =         \end{aligned}</span>     aggiungendo ora e togliendo <span class="math-span">\( k\)</span> all'interno della sommatoria, si ha che     <span class="math-block">\begin{aligned}         = \quad &amp; p \cdot q \cdot \sum_{k = 1}^{+\infty} (k^2 - k + k) \cdot q^{k - 2} &amp; =     \end{aligned}</span>     e dividendo la sommatoria     <span class="math-block">\begin{aligned}         = \quad &amp; p \cdot q \cdot \left( \sum_{k = 1}^{+\infty} (k^2 - k) \cdot q^{k - 2} + \sum_{k = 1}^{+\infty} k \cdot q^{k - 2} \right) &amp; = \\         = \quad &amp; p \cdot q \cdot \left( \sum_{k = 1}^{+\infty} k \cdot (k - 1) \cdot q^{k - 2} + \sum_{k = 1}^{+\infty} k \cdot q^{k - 2} \right) &amp; = \\         = \quad &amp; p \cdot \left( q \cdot \sum_{k = 1}^{+\infty} k \cdot (k - 1) \cdot q^{k - 2} + q \cdot \sum_{k = 1}^{+\infty} k \cdot q^{k - 2} \right) &amp; = \\         = \quad &amp; p \cdot \left( q \cdot \sum_{k = 1}^{+\infty} k \cdot (k - 1) \cdot q^{k - 2} + \sum_{k = 1}^{+\infty} k \cdot q^{k - 1} \right) &amp; =     \end{aligned}</span>     che sono rispettivamente la derivata seconda e la derivata prima     <span class="math-block">\begin{aligned}         = \quad &amp; p \cdot \left( q \cdot \sum_{k = 1}^{+\infty} \frac{d}{dq^2} q^k + \sum_{k = 1}^{+\infty} \frac{d}{dq} q^k \right) &amp; = \\         = \quad &amp; p \cdot \left( q \cdot \frac{d}{dq^2} \left( \sum_{k = 1}^{+\infty} q^k  \right) + \frac{d}{dq} \left( \sum_{k = 1}^{+\infty} q^k \right) \right) &amp; = \\         = \quad &amp; p \cdot \left( q \cdot \frac{d}{dq^2} (1 - q)^{-1} + \frac{d}{dq} (1 - q)^{-1} \right) &amp; = \\         = \quad &amp; p \cdot \left( q \cdot 2 \cdot (1 - q)^{-3} + (1 - q)^{-2} \right) &amp; = \\         = \quad &amp; p \cdot \left( \frac{2q}{(1 - q)^3} + \frac{1}{(1 - q)^2} \right) &amp; = \\         = \quad &amp; p \cdot \left( \frac{2q + 1 - q}{(1 - q)^3} \right) &amp; = \\         = \quad &amp; p \cdot \frac{q + 1}{p^3} &amp; = \\         = \quad &amp; \frac{q + 1}{p^2}     \end{aligned}</span>     per cui si ha che <span class="math-span">\( E\left[ X^2 \right] = \frac{1 + q}{p^2}\)</span>.     <br/>     Calcolando quindi la varianza si ha che     <span class="math-block">\[         \begin{array}{ccl}             Var(X) &amp; = &amp; E\left[ X^2 \right] - \left( E[X] \right)^2 \\             &amp; = &amp; \frac{1 + q}{p^2} - \left( \frac{1}{p}\right)^2 \\             &amp; = &amp; \frac{1 + q}{p^2} - \frac{1}{p^2} \\             &amp; = &amp; \frac{q}{p^2}         \end{array}       \]</span></div></div><div class="definition environment" id="def2-31"><h2 class="environment-title">Definizione - Variabile casuale binomiale negativa</h2><div class="environment-body">     Una variabile casuale discreta <span class="math-span">\( X\)</span> si definisce binomiale negativa di parametri <span class="math-span">\( r\)</span> e <span class="math-span">\( p\)</span> (<span class="math-span">\( X \sim NB(r, p)\)</span>) se, considerando il caso in cui si ripete un esperimento in maniera identica e indipendente fino ad ottenere per <span class="math-span">\( r\)</span> volte (anche non consecutive) un certo esito <span class="math-span">\( A\)</span> di probabilità <span class="math-span">\( p\)</span>, <span class="math-span">\( X\)</span> rappresenta il numero di esperimenti eseguiti per aver ottenuto <span class="math-span">\( r\)</span> successi.     <br/>     Si ha quindi che <span class="math-span">\( X\)</span> è caratterizzata da un numero infinito di valori (a partire da <span class="math-span">\( r\)</span>), ovvero     <span class="math-block">\[         X \in \{ r, \ldots, +\infty \}       \]</span><span class="inner-title"><span class="math-span">\( P(X = k)\)</span></span>     Considerando un valore <span class="math-span">\( k \in \{ r, \ldots, +\infty \}\)</span>, calcolare <span class="math-span">\( P(X = k)\)</span> equivale a calcolare la probabilità che al <span class="math-span">\( k\)</span>-esimo esperimento <span class="math-span">\( A\)</span> si sia verificato per <span class="math-span">\( r\)</span> volte.     Dato che al <span class="math-span">\( k\)</span>-esimo tentativo e certo che si abbia un successo, si ha che ciò equivale all'intersezione  degli eventi      <span class="math-block">\[         B = \text{"in $(k - 1)$ esperimenti si hanno $r - 1$ esiti favorevoli"}       \]</span>      e     <span class="math-block">\[         C = \text{"nel $k$-esimo esperimento si ha un esito favorevole"}       \]</span>     ovvero      <span class="math-block">\[         P(X = k) = P(B \cap C) \quad \underset{\text{eventi indip.}}{\implies} \quad P(X = k) = P(B) \cdot P(C)     \]</span>     Ora, considerando che per definizione <span class="math-span">\( P(C) = p\)</span> e che l'evento <span class="math-span">\( B\)</span> può essere modellato da una v. c. binomiale <span class="math-span">\( Y \sim B(k - 1, r - 1)\)</span>, si ha che     <span class="math-block">\[         \begin{array}{ccl}             P(X = k) &amp; = &amp; P(B) \cdot P(C) \\             &amp; = &amp; \left( C_{k - 1, r - 1} \cdot p^{r - 1} \cdot q^{k - 1 - (r - 1)} \right) \cdot p \\             &amp; = &amp; C_{k - 1, r - 1} \cdot p^{r} \cdot q^{k - r}         \end{array}     \]</span><span class="inner-title">Valore atteso</span>     Il valore atteso di una v. c. binomiale negativa di parametri <span class="math-span">\( r\)</span> e <span class="math-span">\( k\)</span> risulta essere particolarmente complicato da calcolare da definizione.      <br/>     È però possibile considerare che <span class="math-span">\( X\)</span> sia uguale alla somma di <span class="math-span">\( r\)</span> variabili casuali geometriche, ovvero     <span class="math-block">\[         X = \sum_{i = 1}^r Y_i \qquad \text{con} \ Y_i \sim G(p) \ \text{indipendenti}       \]</span>     Per calcolare <span class="math-span">\( E[X]\)</span> si avrà quindi     <span class="math-block">\[         \begin{array}{ccl}             E[X] &amp; = &amp; E\left[ \sum_{i = 1}^r Y_i \right] \\             &amp; = &amp; \sum_{i = 1}^r E[Y_i] \\             &amp; = &amp; \sum_{i = 1}^r \frac{1}{p} \\             &amp; = &amp; \frac{r}{p}         \end{array}     \]</span><span class="inner-title">Varianza</span>     La varianza di una v. c. binomiale negativa di parametri <span class="math-span">\( r\)</span> e <span class="math-span">\( k\)</span>, considerando che <span class="math-span">\( X\)</span> è somma di <span class="math-span">\( r\)</span> variabili casuali geometriche e ogni esperimento è svolto in maniera indipendente per definizione, si ha che è uguale a      <span class="math-block">\[         \begin{array}{ccl}             Var(X) &amp; = &amp; Var\left( \sum_{i = 1}^r X_i \right) \\             &amp; \underset{\text{indip.}}{=} &amp; \sum_{i = 1}^r Var(X_i) \\             &amp; = &amp; \sum_{i = 1}^r \frac{q}{p^2} \\             &amp; = &amp; \frac{r \cdot q}{p^2}         \end{array}     \]</span></div></div><div class="definition environment" id="def2-32"><h2 class="environment-title">Definizione - Variabile casuale di Poisson (o degli elementi rari)</h2><div class="environment-body">     Una variabile casuale discreta <span class="math-span">\( X\)</span> si definisce di Poisson di parametro <span class="math-span">\( \lambda\)</span> (<span class="math-span">\( X \sim Po(\lambda)\)</span>) se, considerando il caso di una serie di esiti <span class="math-span">\( A\)</span> che si verificano in media un numero <span class="math-span">\( \lambda\)</span> di volte in un dato intervallo di tempo, <span class="math-span">\( X\)</span> rappresenta il numero di volte che tali esiti si verificano.     <br/>     Si ha quindi che <span class="math-span">\( X\)</span> è caratterizzata da un numero infinito di valori, ovvero     <span class="math-block">\[         X \in \{ 0, 1, \ldots, +\infty \}      \]</span><span class="inner-title"><span class="math-span">\( P(X = k)\)</span></span>     Considerando un valore <span class="math-span">\( k \in \{ 0, 1, \ldots, +\infty \}\)</span>, si definisce <span class="math-span">\( P(X = k)\)</span> come     <span class="math-block">\[         P(X = k) = \frac{\lambda^k}{k!} \cdot \mathrm{e}^{-\lambda}       \]</span><span class="inner-title">Funzione generatrice dei momenti</span>     La funzione generatrice dei momenti di una v. c. di Poisson di parametro <span class="math-span">\( \lambda\)</span> è uguale a      <span class="math-block">\begin{aligned}         &amp; \phi(t) = E[\mathrm{e}^{t \cdot X}] &amp; = \\     \end{aligned}</span>     si ha che è uguale a     <span class="math-block">\begin{aligned}         = \quad &amp; \sum_{k = 0}^{+\infty} \mathrm{e}^{t \cdot k} \cdot p(k) &amp; = \\         = \quad &amp; \sum_{k = 0}^{+ \infty} \mathrm{e}^{t \cdot k} \cdot \frac{\lambda^k}{k!} \cdot \mathrm{e^{-\lambda}} &amp; = \\         = \quad &amp;  \mathrm{e^{-\lambda}} \cdot \sum_{k = 0}^{+ \infty} \mathrm{e}^{t \cdot k} \cdot \frac{\lambda^k}{k!}&amp; =     \end{aligned}</span>     e, considerando la serie esponenziale per cui si ha     <span class="math-block">\[         \sum_{k = 0}^{+\infty} \frac{a^k}{k!} = \mathrm{e}^a       \]</span>     si ha che     <span class="math-block">\begin{aligned}         = \quad &amp;  \mathrm{e^{-\lambda}} \cdot \sum_{k = 0}^{+ \infty} \frac{(\mathrm{e}^{t} \cdot \lambda)^k}{k!} &amp; = \\         = \quad &amp;  \mathrm{e^{-\lambda}} \cdot \mathrm{e}^{\lambda \cdot \mathrm{e}^t} &amp; = \\         = \quad &amp;  \mathrm{e}^{-\lambda + \lambda \cdot \mathrm{e}^t} &amp; = \\         = \quad &amp;  \mathrm{e}^{\lambda \cdot (-1 + \mathrm{e}^t)} &amp; =     \end{aligned}</span>     per cui si ha che <span class="math-span">\( \phi(t) = \mathrm{e}^{\lambda \cdot (\mathrm{e}^t - 1)}\)</span>.     <span class="inner-title">Valore atteso</span>     Il valore atteso di una v. c. di Poisson di parametro <span class="math-span">\( \lambda\)</span> è calcolabile come     <span class="math-block">\[         \begin{array}{ccl}             \frac{d}{dt} \phi(t) \mid_{t = 0} &amp; = &amp; \mathrm{e}^{\lambda \cdot (\mathrm{e}^t - 1)} \cdot \frac{d}{dt} (\lambda \cdot (\mathrm{e}^t - 1)) \mid_{t = 0} \\             &amp; = &amp; \mathrm{e}^{\lambda \cdot (\mathrm{e}^t - 1)} \cdot \lambda \cdot \mathrm{e}^t \mid_{t = 0} \\             &amp; = &amp;  \lambda           \end{array}     \]</span><span class="inner-title">Varianza</span>     La varianza di una v. c. di Poisson di parametro <span class="math-span">\( \lambda\)</span> è calcolabile come     <span class="math-block">\[         Var(X) = E\left[ X^2 \right] - (E[X])^2     \]</span>     È necessario calcolare quindi <span class="math-span">\( E\left[ X^2 \right]\)</span> che è uguale a     <span class="math-block">\[         \begin{array}{ccl}             E\left[ X^2 \right] &amp; = &amp; \frac{d}{dt} \left( \mathrm{e}^{\lambda \cdot (\mathrm{e}^t - 1)} \cdot \lambda \cdot \mathrm{e}^t \right) \mid_{t = 0} \\             &amp; = &amp; \lambda^2 + \lambda          \end{array}       \]</span>     Calcolando quindi la varianza si ha che     <span class="math-block">\[         \begin{array}{ccl}             Var(X) &amp; = &amp; E\left[ X^2 \right] - (E[X])^2 \\             &amp; = &amp; \lambda^2 + \lambda - \lambda^2 \\             &amp; = &amp; \lambda         \end{array}     \]</span></div></div><div class="demonstration environment" id="dem2-9"><h2 class="environment-title">Dimostrazione - Proprietà di riproducibilità di variabili casuali di Poisson</h2><div class="environment-body">     Data la proposizione     <div class="proposition environment"><h3 class="environment-title">Enunciato</h3><div class="environment-body">         Considerando due variabili casuali di Poisson indipendenti <span class="math-span">\( X\)</span> e <span class="math-span">\( Y\)</span>, tali che <span class="math-span">\( X \sim Po(\lambda_1)\)</span> e <span class="math-span">\( Y \sim Po(\lambda_2)\)</span>, si ha che         <span class="math-block">\[             X + Y \sim Po(\lambda_1 + \lambda_2)           \]</span></div></div><div class="proof environment"><h3 class="environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="toggle_proof(event)">visibility_off</span></h3><div class="environment-body hyde">         Per dimostrare questa proposizione, considerando le funzioni generatrici dei momenti, si dovrebbe avere che         <span class="math-block">\[             \phi_{X + Y}(t) = \mathrm{e}^{(\lambda_1 + \lambda_2) \cdot (\mathrm{e}^t - 1)}         \]</span>         Considerando la definizione, si ha che         <span class="math-block">\begin{aligned}             &amp; \phi_{X + Y}(t) = E\left[ \mathrm{e}^{t \cdot (X + Y)} \right] &amp; \iff \\             &amp; \phi_{X + Y}(t) = E\left[ \mathrm{e}^{t \cdot X + t \cdot Y} \right] &amp; \iff \\             &amp; \phi_{X + Y}(t) = E\left[ \mathrm{e}^{t \cdot X} \cdot \mathrm{e}^{t \cdot Y} \right] &amp; \iff         \end{aligned}</span>         e per l'indipendenza delle variabili, si ha che         <span class="math-block">\begin{aligned}             &amp; \phi_{X + Y}(t) = E\left[ \mathrm{e}^{t \cdot X} \right] \cdot E\left[ \mathrm{e}^{t \cdot Y} \right] &amp; \iff \\             &amp; \phi_{X + Y}(t) = \mathrm{e}^{lambda_1 \cdot (\mathrm{e}^t - 1)} \cdot  \mathrm{e}^{\lambda_2 \cdot (\mathrm{e}^t - 1)}  &amp; \iff \\             &amp; \phi_{X + Y}(t) = \mathrm{e}^{(\lambda_1 + \lambda_2) \cdot (\mathrm{e}^t - 1)} &amp;         \end{aligned}</span>         che dimostra la proposizione.     </div></div></div></div><div class="myexample environment" id="example28"><h2 class="environment-title">Esempio - Utilizzo di una v. c. di Poisson</h2><div class="environment-body collapsed">     Considerando una compagnia di assicurazioni che riceve in media <span class="math-span">\( 5\)</span> richieste di rimborso in un giorno, calcolare:     <ul class="list-container"><li class="list-item">la probabilità di ricevere meno di <span class="math-span">\( 3\)</span> richieste in un giorno;         </li><li class="list-item">la probabilità di ricevere <span class="math-span">\( 4\)</span> richieste in cinque giorni;     </li></ul><span class="inner-title">Probabilità di ricevere meno di <span class="math-span">\( 3\)</span> richieste in un giorno</span>     Per risolvere questo problema, modelliamo il caso con una variabile casuale di Poisson di parametro <span class="math-span">\( 5\)</span>, ovvero     <span class="math-block">\[         X \sim Po(5)       \]</span>     Dove <span class="math-span">\( X\)</span> indica il numero di richieste ricevute in un giorno.     Si vuole calcolare <span class="math-span">\( P(X \lt 3)\)</span>, ovvero     <span class="math-block">\[         P(X \lt 3) = P(X = 0) + P(X = 1) + P(X = 2)       \]</span>     e, considerando che <span class="math-span">\( P(X = k) = \frac{\lambda^k}{k!} \cdot \mathrm{e}^{-\lambda}\)</span>, si ha che     <span class="math-block">\[         \begin{array}{ccl}             P(X \lt 3) &amp; = &amp; P(X = 0) + P(X = 1) + P(X = 2)  \\             &amp; = &amp; \frac{5^0}{0!} \cdot \mathrm{e}^{-5} + \frac{5^1}{1!} \cdot \mathrm{e}^{-5} + \frac{5^2}{2!} \cdot \mathrm{e}^{-5} \\             &amp; = &amp; \frac{1}{\mathrm{e}^5} + \frac{5}{\mathrm{e}^5} + \frac{25}{2 \mathrm{e}^{5}} \\             &amp; = &amp; \frac{1}{\mathrm{e}^{5}} \cdot (1 + 5 + \frac{25}{2}) \\             &amp; = &amp; \frac{37}{2} \cdot \mathrm{e}^{-5}         \end{array}     \]</span><span class="inner-title">Probabilità di ricevere <span class="math-span">\( 4\)</span> richieste in cinque giorni</span>     Per risolvere questo problema, è possibile considerare che ogni giorno sia modellato da una variabile casuale di Poisson di parametro <span class="math-span">\( 5\)</span> (<span class="math-span">\( X_i \sim Po(5)\)</span> con <span class="math-span">\( i \in \{ 1, 2, 3, 4, 5 \}\)</span>).     In cinque giorni, si avrebbe che la variabile     <span class="math-block">\[         (Y = X_1 + X_2 + X_3 + X_4 + X_5) = \text{"numero di richieste in cinque giorni"}      \]</span>     Sappiamo, ipotizzando che siano eventi indipendenti, che per la proprietà di riproducibilità vale     <span class="math-block">\[         Y \sim Po(5 + 5 + 5 + 5 + 5)     \]</span>     Si avrà quindi che <span class="math-span">\( P(Y = 4)\)</span> sarà uguale a     <span class="math-block">\[         \begin{array}{ccl}             P(Y = 4) &amp; = &amp; \frac{\lambda^k}{k!} \cdot \mathrm{e}^{-\lambda} \\             &amp; = &amp; \frac{25^4}{4!} \cdot \mathrm{e}^{-25}         \end{array}       \]</span></div><div class="environment-tail"><span class="material-symbols-outlined body-visibility-icon" onclick="expand_environment(event)">expand_more</span></div></div><div class="myexample environment" id="example29"><h2 class="environment-title">Esempio - Determinare probabilità condizionata di v. c. di Poisson</h2><div class="environment-body collapsed">     Considerando due variabili casuali di Poisson <span class="math-span">\( X \sim Po(\lambda_1)\)</span> e <span class="math-span">\( Y \sim Po(\lambda_2)\)</span> indipendenti, verificare che     <span class="math-block">\[         P(X = k \mid X + Y = n) \qquad \text{con} \ k \in \{ 1, \ldots, n \}     \]</span>     ha una distribuzione binomiale e determinare il valore di <span class="math-span">\( p\)</span>.     <br/>     Considerando la definizione di probabilità condizionata, si ha che     <span class="math-block">\begin{aligned}         &amp; P(X = k \mid X + Y = n) = \frac{P(X = k \cap X + Y = n)}{P(X + Y = n)} &amp; \iff     \end{aligned}</span>     Dato che <span class="math-span">\( X = k\)</span>, è possibile sostituirlo al valore nell'intersezione dei due eventi, ovvero     <span class="math-block">\begin{aligned}         &amp; P(X = k \mid X + Y = n) = \frac{P(X = k \cap Y = n - k)}{P(X + Y = n)} &amp; \iff     \end{aligned}</span>     e per l'indipendenza degli eventi     <span class="math-block">\begin{aligned}         &amp; P(X = k \mid X + Y = n) = \frac{P(X = k) \cdot P(Y = n - k)}{P(X + Y = n)} &amp;     \end{aligned}</span>     è ora possibile calcolare le singole probabilità.     <br/>     Considerando che <span class="math-span">\( X\)</span> è una variabile casuale di Poisson di parametro <span class="math-span">\( \lambda_1\)</span>, si ha che     <span class="math-block">\[         P(X = k) = \frac{(\lambda_1)^k}{k!} \cdot \mathrm{e}^{-\lambda_1}       \]</span>     Considerando che <span class="math-span">\( Y\)</span> è una variabile di Poisson di parametro <span class="math-span">\( \lambda_2\)</span>, si ha che      <span class="math-block">\[         P(X = n - k) =  \frac{(\lambda_2)^{n - k}}{(n - k)!} \cdot \mathrm{e}^{-\lambda_2}       \]</span>     Considerando ora il principio di riproducibilità, si ha che <span class="math-span">\( X + Y\)</span> è una variabile casuale di parametro <span class="math-span">\( \lambda_1 + \lambda_2\)</span>, si ha che     <span class="math-block">\[         P(X + Y = n) = \frac{(\lambda_1 + \lambda_2)^n}{n!} \cdot \mathrm{e}^{-(\lambda_1 + \lambda_2)}       \]</span>     Unendo ciò, si avrà che     <span class="math-block">\[         \begin{array}{ccl}             P(X = k \mid X + Y = n) &amp; = &amp; \frac{P(X = k) \cdot P(Y = n - k)}{P(X + Y = n)} \\             &amp; = &amp; \frac{\frac{(\lambda_1)^k}{k!} \cdot \mathrm{e}^{-\lambda_1} \cdot \frac{(\lambda_2)^{n - k}}{(n - k)!} \cdot \mathrm{e}^{-\lambda_2}}{\frac{(\lambda_1 + \lambda_2)^n}{n!} \cdot \mathrm{e}^{-(\lambda_1 + \lambda_2)}} \\             &amp; = &amp; \frac{\frac{(\lambda_1)^k}{k!} \cdot \frac{(\lambda_2)^{n - k}}{(n - k)!}}{\frac{(\lambda_1 + \lambda_2)^n}{n!}} \\             &amp; = &amp; \frac{(\lambda_1)^k}{k!} \cdot \frac{(\lambda_2)^{n - k}}{(n - k)!} \cdot \frac{n!}{(\lambda_1 + \lambda_2)^n} \\             &amp; = &amp; \frac{n!}{k! \cdot (n - k)!} \cdot \frac{(\lambda_2)^{n - k} \cdot (\lambda_1)^k}{(\lambda_1 + \lambda_2)^n} \\             &amp; = &amp; C_{n, k} \cdot \frac{(\lambda_2)^{n - k} \cdot (\lambda_1)^k}{(\lambda_1 + \lambda_2)^{n - k} \cdot (\lambda_1 + \lambda_2)^{k}} \\             &amp; = &amp; C_{n, k} \cdot \left( \frac{\lambda_2}{\lambda_1 + \lambda_2} \right)^{n - k} \cdot \left( \frac{\lambda_1}{\lambda_1 + \lambda_2} \right)^{k}         \end{array}     \]</span>     e considerando <span class="math-span">\( p = \frac{\lambda_1}{\lambda_1 + \lambda_2}\)</span>, si ha che è proprio uguale ad una distribuzione binomiale di parametri <span class="math-span">\( n\)</span> e <span class="math-span">\( p\)</span>. </div><div class="environment-tail"><span class="material-symbols-outlined body-visibility-icon" onclick="expand_environment(event)">expand_more</span></div></div>
            </article>
            <nav class="buttons-container content-width">
                <a class="navigation-button previous" href="funzione-generatrice-di-momenti.html" rel="nofollow"><span>Funzione generatrice di momenti</span></a>
                <a class="navigation-button next" href="modelli-di-variabili-casuali-continue.html" rel="nofollow"><span>Modelli di variabili casuali continue</span></a>
            </nav>
        </section>
        <div class="scroll-to-bottom-button" onclick="scroll_to_bottom()">
            <span class="material-symbols-outlined">
                keyboard_double_arrow_down
            </span>
        </div>
        <footer class="footer-wrapper">
            <div class="copyright-wrapper">
                <span> &copy; Copyright 2023</span> /
                <span>made by lorenzoarlo</span>
            </div>
            /
            <div class="privacy-wrapper">
                <span><a href="https://lorenzoarlo.github.io/privacy-and-cookies/" rel="nofollow">Pannello preferenze cookie</a></span> /
                <span><a href="https://lorenzoarlo.github.io/privacy-and-cookies/privacy-policy.html" rel="nofollow" target="_blank" >Privacy Policy</a></span>
            </div>
        </footer>
    </div>
</body>
</html>