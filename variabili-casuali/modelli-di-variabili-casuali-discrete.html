<!DOCTYPE html>
<html lang="IT">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="stylesheet" href="../styles/style.css" />
    <link rel="stylesheet" href="../styles/index-style.css" />
    <link rel="stylesheet" href="../styles/content-style.css" />
    <link rel="icon" type="image/x-icon" href="../resources/favicon.ico">
    <link rel="apple-touch-icon" href="../resources/favicon.png" />
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-78NHLXDQD8"></script>
    <script src="../scripts/script.js"></script>
    <style>:root { --bg-clr: #E34234; --fg-clr: #262626; }</style>
    <meta name="theme-color" content="#E34234" />
    <meta name="keywords" content="matematica applicata" />
    <meta name="description" content="Il seguente sito contiene gli appunti e le definizioni del corso 'Matematica applicata'.">
    <meta name="robots" content="index">
    <script defer async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <title>Matematica applicata - Variabili casuali - Modelli di variabili casuali discrete</title>
</head>
<body>
    <div class="container">
        <header class="header-wrapper">
            <div class="title-wrapper">
                <span class="title">
                    Matematica applicata
                </span>
                <span class="subtitle">
                    by lorenzoarlo
                </span>
            </div>
            <span class="page-title">
                Variabili casuali
            </span>
        </header>
        <section class="content-wrapper">
            <h1 class="section-title content-width">Modelli di variabili casuali discrete</h1>
            <article class="content-container content-width">
                <div class="definition environment" id="def2-28"><h2 class="environment-title">Definizione - Variabile casuale di Bernoulli (o Bernoulliane)</h2><div class="environment-body">     Una variabile casuale discreta <span class="math-span">\( X\)</span> si definisce di Bernoulli di parametro <span class="math-span">\( p\)</span> (<span class="math-span">\( X \sim Be(p)\)</span>) se è caratterizzata da due soli valori (detti di "successo" o "fallimento"), ovvero     <span class="math-block">\[         X \in \{ 0, 1 \}       \]</span>     dove      <span class="math-block">\[         P(X = 1) = p     \]</span>     e che      <span class="math-block">\[         P(X = 0) = q = 1 - p       \]</span>     La variabile casuale si presenterà quindi come     <span class="math-block">\[         X = \left\{              \begin{array}{ll}                 1 &amp; \text{se si verifica l'evento} \ A \\                 0 &amp; \text{se non si verifica l'evento} \ A              \end{array}         \right.     \]</span><span class="inner-title">Valore atteso</span>     Il valore atteso di una v. c. di Bernoulli di parametro <span class="math-span">\( p\)</span>, calcolabile come     <span class="math-block">\[         \begin{array}{ccl}             E[X] &amp; = &amp; 0 \cdot q + 1 \cdot p \\             &amp; = &amp; p         \end{array}     \]</span>     è uguale a <span class="math-span">\( p\)</span>.     <span class="inner-title">Momento di ordine <span class="math-span">\( n\)</span></span>     Il momento di ordine <span class="math-span">\( n\)</span> di una v. c. di Bernoulli di parametro <span class="math-span">\( p\)</span>, calcolabile come     <span class="math-block">\[         \begin{array}{ccl}             E[X^n] &amp; = &amp; 0^n \cdot q + 1^n \cdot p \\             &amp; = &amp; p         \end{array}     \]</span>     è uguale a <span class="math-span">\( p\)</span>, per ogni <span class="math-span">\( n \in \mathbb{N}\)</span>.     <span class="inner-title">Varianza</span>     La varianza di una v. c. di Bernoulli di parametro <span class="math-span">\( p\)</span>, calcolabile come     <span class="math-block">\[         \begin{array}{ccl}             Var(X) &amp; = &amp; E[X^2] - \left( E[X] \right)^2 \\             &amp; = &amp; p - p^2 \\             &amp; = &amp; p \cdot (1 - p) \\             &amp; = &amp; p \cdot q         \end{array}     \]</span>     è uguale a <span class="math-span">\( p \cdot q\)</span>.     <span class="inner-title">Funzione generatrice dei momenti</span>     La funzione generatrice dei momenti di una v. c. di Bernoulli di parametro <span class="math-span">\( p\)</span>, calcolabile come     <span class="math-block">\[         \begin{array}{ccl}             \phi(t) &amp; = &amp; E\left[\mathrm{e}^{t \cdot X}\right] \\             &amp; = &amp; \sum_{k = 1}^2 \mathrm{e}^{t \cdot x_k} \cdot p(x_k) \\             &amp; = &amp; \mathrm{e}^{t \cdot 0} \cdot q + \mathrm{e}^{t \cdot 1} \cdot p \\             &amp; = &amp; q + \mathrm{e}^t \cdot p         \end{array}     \]</span>     è uguale a <span class="math-span">\( q + \mathrm{e}^t \cdot p\)</span>. </div></div><div class="myexample environment" id="example26"><h2 class="environment-title">Esempio - Determinare funzioni di massa e covarianza per una coppia di v. c. discrete</h2><div class="environment-body collapsed">     Considerando una variabile casuale di Bernoulli <span class="math-span">\( X\)</span> di parametro <span class="math-span">\( \frac{1}{2}\)</span> (ovvero, <span class="math-span">\( X \sim Be\left(\frac{1}{2}\right)\)</span>) e <span class="math-span">\( Y\)</span> una variabile casuale discreta tale che      <span class="math-block">\[         P(Y = 1 \ \mid \ X = 0) = \varepsilon         \]</span>     ovvero ci è data la che <span class="math-span">\( Y\)</span> sia <span class="math-span">\( 1\)</span> dato <span class="math-span">\( X\)</span> uguale a <span class="math-span">\( 0\)</span> (ovvero vi è probabilità condizionata) e che     <span class="math-block">\[         P(Y = 0 \ \mid \ X = 1) = 2 \cdot \varepsilon             \]</span>     con <span class="math-span">\( \varepsilon \in ] 0, \frac{1}{2} [\)</span>, determinare:     <ul class="list-container"><li class="list-item">la funzione di massa congiunta e le funzioni di massa marginale;         </li><li class="list-item">la covarianza di <span class="math-span">\( X\)</span> e <span class="math-span">\( Y\)</span>;         </li><li class="list-item">i valori di <span class="math-span">\( \varepsilon\)</span> (se esistono) per cui <span class="math-span">\( X\)</span> e <span class="math-span">\( Y\)</span> sono scorrelate (ovvero <span class="math-span">\( Cov(X, Y) = 0\)</span>)     </li></ul><span class="inner-title">Funzione di massa congiunta e funzioni di massa marginale</span>     Considerando tali variabili, si ha che conosciamo già la funzione di massa marginale <span class="math-span">\( p_X\)</span> (dato che è una variabile di Bernoulli), in quanto sappiamo che     <span class="math-block">\[         p_X(1) = \frac{1}{2}     \]</span>     e che      <span class="math-block">\[         p_X(0) = 1 - \frac{1}{2} = \frac{1}{2}     \]</span>     e, unendo ciò alle informazioni date dalla consegna, è possibile calcolare     <ul class="list-container"><li class="list-item">il valore di <span class="math-span">\( p(1, 0)\)</span> che è uguale a         <span class="math-block">\[             \begin{array}{ccl}                 P(X = 1 \ \wedge \ Y = 0) &amp; = &amp; P(Y = 0 \ \mid \ X = 1) \cdot p_X(1) \\                 &amp; = &amp; 2 \cdot \varepsilon \cdot \frac{1}{2} \\                 &amp; = &amp; \varepsilon             \end{array}         \]</span></li><li class="list-item">il valore di <span class="math-span">\( p(0, 1)\)</span> che è uguale a         <span class="math-block">\[             \begin{array}{ccl}                 P(X = 0 \ \wedge \ Y = 1) &amp; = &amp; P(Y = 1 \ \mid \ X = 0) \cdot p_X(0) \\                 &amp; = &amp; \varepsilon \cdot \frac{1}{2} \\                 &amp; = &amp; \frac{\varepsilon}{2}             \end{array}         \]</span></li></ul>     da cui è infine possibile calcolare anche i rimanenti valori:     <ul class="list-container"><li class="list-item">il valore di <span class="math-span">\( p(0, 0)\)</span> che è uguale a         <span class="math-block">\[             \begin{array}{ccl}                 P(X = 0 \ \wedge \ Y = 0) &amp; = &amp;  p_X(0) - p(0, 1) \\                 &amp; = &amp; \frac{1}{2} - \frac{\varepsilon}{2}             \end{array}         \]</span></li><li class="list-item">il valore di <span class="math-span">\( p(1, 1)\)</span> che è uguale a         <span class="math-block">\[             \begin{array}{ccl}                 P(X = 1 \ \wedge \ Y = 1) &amp; = &amp;  p_X(1) - p(1, 0) \\                 &amp; = &amp; \frac{1}{2} - \varepsilon             \end{array}         \]</span></li></ul>     Da ciò è anche possibile ottenere la funzione <span class="math-span">\( p_Y\)</span> come     <ul class="list-container"><li class="list-item"><span class="math-span">\( p_Y(0)\)</span> è uguale a         <span class="math-block">\[             \begin{array}{ccl}                 p_Y(0) &amp; = &amp; p(0, 0) + p(1, 0) \\                 &amp; = &amp; \frac{1}{2} - \frac{\varepsilon}{2} + \varepsilon \\                 &amp; = &amp; \frac{1}{2} + \frac{\varepsilon}{2}             \end{array}         \]</span></li><li class="list-item"><span class="math-span">\( p_Y(1)\)</span> è uguale a         <span class="math-block">\[             \begin{array}{ccl}                 p_Y(1) &amp; = &amp; p(0, 1) + p(1, 1) \\                 &amp; = &amp; \frac{\varepsilon}{2} + \frac{1}{2} - \varepsilon \\                 &amp; = &amp; \frac{1}{2} - \frac{\varepsilon}{2}             \end{array}         \]</span></li></ul>     È infine possibile ottenere     <div class="image-environment"><div class="image-wrapper spaced-60"><img alt="Immagine" src="../resources/generated-18.png"/></div></div><span class="inner-title">Covarianza</span>     Per calcolare la covarianza di <span class="math-span">\( X\)</span> e <span class="math-span">\( Y\)</span>, si ha che è necessario calcolare     <span class="math-block">\[         Cov(X, Y) = E[X \cdot Y] - E[X] \cdot E[Y]      \]</span>     Si ha quindi che:     <ul class="list-container"><li class="list-item">il valore atteso <span class="math-span">\( E[X]\)</span> è uguale a <span class="math-span">\( \frac{1}{2}\)</span> in quanto variabile di Bernoulli di parametro <span class="math-span">\( \frac{1}{2}\)</span>;         </li><li class="list-item">il valori atteso <span class="math-span">\( E[Y]\)</span> è uguale         <span class="math-block">\[             \begin{array}{ccl}                 E[Y] &amp; = &amp; 0 \cdot p_Y(0) + 1 \cdot p_Y(1) \\                 &amp; = &amp; \frac{1}{2} - \frac{\varepsilon}{2}             \end{array}         \]</span></li><li class="list-item">il valore atteso <span class="math-span">\( E[X \cdot Y]\)</span> è necessario calcolarlo come         <span class="math-block">\[             \begin{array}{ccl}                 E[X \cdot Y] &amp; = &amp; \sum_{i = 1}^{2} \sum_{j = 1}^{2} x_i \cdot y_j \cdot p(x_i, y_j) \\                 &amp; = &amp; 0 \cdot 0 \cdot p(0, 0) + 0 \cdot 1 \cdot p(0, 1) + 1 \cdot 0 \cdot p(1,0) + 1 \cdot 1 \cdot p(1, 1) \\                 &amp; = &amp; \frac{1}{2} - \varepsilon             \end{array}         \]</span>         dato che non sono indipendenti tra loro generalmente (come è evidente dalla funzione di massa).     </li></ul>     Unendo ciò, si ha che      <span class="math-block">\[         \begin{array}{ccl}             Cov(X, Y) &amp; = &amp; E[X \cdot Y] - E[X] \cdot E[Y] \\             &amp; = &amp; \frac{1}{2} - \varepsilon - \frac{1}{2} \cdot \left( \frac{1}{2} - \frac{\varepsilon}{2} \right) \\             &amp; = &amp; \frac{1}{2} - \varepsilon - \frac{1}{4} + \frac{\varepsilon}{4} \\             &amp; = &amp; \frac{1}{4} - \frac{3}{4} \cdot \varepsilon         \end{array}       \]</span><span class="inner-title"><span class="math-span">\( \varepsilon\)</span> per cui <span class="math-span">\( X\)</span> e <span class="math-span">\( Y\)</span> sono scorrelate</span>     Per calcolare i valori di <span class="math-span">\( \varepsilon\)</span> per cui <span class="math-span">\( X\)</span> e <span class="math-span">\( Y\)</span> sono scorrelate, si ha che è sufficiente risolvere la seguente equazione in <span class="math-span">\( \varepsilon\)</span>:     <span class="math-block">\begin{aligned}         &amp; \frac{1}{4} - \frac{3}{4} \cdot \varepsilon = 0 &amp; \iff \\         &amp; \frac{1}{4} = \frac{3}{4} \cdot \varepsilon &amp; \iff \\         &amp; \frac{1}{3} = \varepsilon &amp;     \end{aligned}</span>     Dato che <span class="math-span">\( \varepsilon \in ] 0, \frac{1}{2} [\)</span>, si ha che <span class="math-span">\( \varepsilon = \frac{1}{3}\)</span> è valido, rendendo quindi le due variabili scorrelate per tale valore. </div><div class="environment-tail"><span class="material-symbols-outlined body-visibility-icon" onclick="expand_environment(event)">expand_more</span></div></div><div class="myexample environment" id="example27"><h2 class="environment-title">Esempio - Determinare funzioni di massa, valore atteso, varianza e covarianza di v. c. di Bernoulli</h2><div class="environment-body collapsed">     Considerando le variabili casuali di Bernoulli <span class="math-span">\( X \sim Be\left(\frac{3}{4}\right)\)</span> e <span class="math-span">\( Y \sim Be\left(\frac{3}{4}\right)\)</span> calcolare:     <ul class="list-container"><li class="list-item">le funzioni di massa congiunta e marginali;         </li><li class="list-item">il valore atteso <span class="math-span">\( E[X]\)</span> e <span class="math-span">\( E[Y]\)</span>;         </li><li class="list-item">la varianza di <span class="math-span">\( X\)</span> e di <span class="math-span">\( Y\)</span>;         </li><li class="list-item">la covarianza di <span class="math-span">\( X, Y\)</span>;         </li><li class="list-item">il coefficiente di correlazione;         </li><li class="list-item">se sono indipendenti.     </li></ul><span class="inner-title">Funzioni di massa di probabilità</span>     Per calcolare la funzione di massa è necessario considerare che sono variabili casuali di Bernoulli di parametro <span class="math-span">\( \frac{3}{4}\)</span>, ovvero si ha che conosciamo le funzioni di massa marginale, ovvero     <ul class="list-container"><li class="list-item"><span class="math-span">\( p_X(1) = \frac{3}{4}\)</span> e <span class="math-span">\( p_X(0) = 1 - \frac{3}{4} = \frac{1}{4}\)</span>;         </li><li class="list-item"><span class="math-span">\( p_Y(1) = \frac{3}{4}\)</span> e <span class="math-span">\( p_Y(0) = 1 - \frac{3}{4} = \frac{1}{4}\)</span>.     </li></ul>     Analogamente, per calcolare la funzione di massa congiunta è necessario considerare che:     <ul class="list-container"><li class="list-item"><span class="math-span">\( p(X = 1 \ \wedge \ Y = 0)\)</span> è uguale a <span class="math-span">\( 0\)</span>, in quanto è impossibile che coesistano i due eventi;         </li><li class="list-item"><span class="math-span">\( p(X = 0 \ \wedge \ Y = 1)\)</span> è uguale a <span class="math-span">\( 0\)</span>, in quanto è impossibile che coesistano i due eventi;         </li><li class="list-item"><span class="math-span">\( p(X = 0 \ \wedge \ Y = 0)\)</span> è uguale a <span class="math-span">\( p_X(0) - p(0, 1) = \frac{1}{4} - 0\)</span>, ovvero è uguale a <span class="math-span">\( \frac{1}{4}\)</span>;         </li><li class="list-item"><span class="math-span">\( p(X = 1 \ \wedge \ Y = 1)\)</span> è uguale a <span class="math-span">\( p_X(1) - p(1, 0) = \frac{3}{4} - 0\)</span>, ovvero è uguale a <span class="math-span">\( \frac{3}{4}\)</span>.     </li></ul><div class="image-environment"><div class="image-wrapper spaced-60"><img alt="Immagine" src="../resources/generated-19.png"/></div></div><span class="inner-title">Valore atteso <span class="math-span">\( E[X]\)</span> e <span class="math-span">\( E[Y]\)</span></span>     Dato che il valore atteso di una variabile di Bernoulli di parametro <span class="math-span">\( p\)</span> è proprio <span class="math-span">\( p\)</span>, si ha che     <span class="math-block">\[         E[X] = \frac{3}{4}         \]</span>     e     <span class="math-block">\[         E[Y] = \frac{3}{4}       \]</span><span class="inner-title">Varianza di <span class="math-span">\( X\)</span> e di <span class="math-span">\( Y\)</span></span>     Per calcolare la varianza si ha che è necessario calcolare <span class="math-span">\( E[X^2]\)</span>, ovvero     <span class="math-block">\[         \begin{array}{lcl}             E\left[X^2\right] = E\left[Y^2\right]  &amp; = &amp; 0^2 \cdot \frac{1}{4} + 1^2 \frac{3}{4} \\             &amp; = &amp; \frac{3}{4}         \end{array}       \]</span>     da cui      <span class="math-block">\[         \begin{array}{lcl}             Var(X) = Var(Y) &amp; = &amp; E\left[X^2\right] - \left( E[X] \right)^2 \\             &amp; = &amp; \frac{3}{4} - \frac{9}{16} \\             &amp; = &amp; \frac{3}{16}         \end{array}       \]</span><span class="inner-title">Covarianza di <span class="math-span">\( X\)</span> e <span class="math-span">\( Y\)</span></span>     Per calcolare la covarianza delle due variabili, si ha che è uguale a      <span class="math-block">\[         Cov(X, Y) = E[X \cdot Y] - E[X] \cdot E[Y]       \]</span>     È quindi necessario calcolare <span class="math-span">\( E[X \cdot Y]\)</span> che è uguale a     <span class="math-block">\[         \begin{array}{lcl}             E[X \cdot Y] &amp; = &amp; x_1 \cdot y_1 \cdot p(x_1, y_1) + x_1 \cdot y_2 \cdot p(x_1, y_2) + x_2 \cdot y_1 \cdot p(x_2, y_1) + x_2 \cdot y_2 \cdot p(x_2, y_2) \\             &amp; = &amp; 0 \cdot 0 \cdot 0 \cdot p(0,0) + 0 \cdot 1 \cdot p(0,1) + 1 \cdot 0 \cdot p(1,0) + 1 \cdot 1 \cdot p(1, 1) \\             &amp; = &amp; \frac{3}{4}         \end{array}     \]</span>     da cui     <span class="math-block">\[         \begin{array}{lcl}             Cov(X, Y) &amp; = &amp;  E[X \cdot Y] - E[X] \cdot E[Y] \\             &amp; = &amp; \frac{3}{4} - \frac{3}{4} \cdot \frac{3}{4}\\             &amp; = &amp; \frac{3}{4} - \frac{9}{16} \\             &amp; = &amp; \frac{3}{16}         \end{array}     \]</span><span class="inner-title">Coefficiente di correlazione</span>     Per calcolare il coefficiente di correlazione è necessario calcolare     <span class="math-block">\[         \begin{array}{lcl}             Corr(X, Y) &amp; = &amp; \frac{Cov(X, Y)}{\sqrt{Var(X) \cdot Var(Y)}} \\             &amp; = &amp; \frac{\frac{3}{16}}{\sqrt{\frac{3}{16} \cdot \frac{3}{16}}} \\             &amp; = &amp; 1         \end{array}       \]</span><span class="inner-title">Indipendenza</span>     Per verificare l' indipendenza è sufficiente verificare che la covarianza sia uguale a <span class="math-span">\( 0\)</span>: in questo caso non lo è quindi si ha che <span class="math-span">\( X\)</span> e <span class="math-span">\( Y\)</span> sono dipendenti. </div><div class="environment-tail"><span class="material-symbols-outlined body-visibility-icon" onclick="expand_environment(event)">expand_more</span></div></div><div class="definition environment" id="def2-29"><h2 class="environment-title">Definizione - Variabile casuale binomiale</h2><div class="environment-body">     Una variabile casuale discreta <span class="math-span">\( X\)</span> si definisce binomiale di parametri <span class="math-span">\( n\)</span> e <span class="math-span">\( p\)</span> (<span class="math-span">\( X \sim B(n, p)\)</span>) se, considerando il caso in cui si ripete un esperimento <span class="math-span">\( n\)</span> volte in maniera identica e indipendente e la probabilità che un certo esito <span class="math-span">\( A\)</span> si verifichi sia <span class="math-span">\( p\)</span>, <span class="math-span">\( X\)</span> rappresenta il numero di esperimenti (sugli <span class="math-span">\( n\)</span> totali) in cui <span class="math-span">\( A\)</span> si è verificato.     <br/>     Si ha quindi che <span class="math-span">\( X\)</span> è caratterizzata da <span class="math-span">\( n + 1\)</span> valori, ovvero     <span class="math-block">\[         X \in \{ 0, 1, \ldots, n \}       \]</span><span class="inner-title"><span class="math-span">\( P(X = k)\)</span></span>     Considerando un valore <span class="math-span">\( k \in \{ 0, 1, \ldots, n \}\)</span>, per calcolare <span class="math-span">\( P(X = k)\)</span> si ha che è necessario considerare che la probabilità che <span class="math-span">\( A\)</span> si verifichi <span class="math-span">\( k\)</span> volte è data dal prodotto tra:     <ul class="list-container"><li class="list-item">il numero totale di possibili combinazioni in cui si hanno <span class="math-span">\( k\)</span> successi, ovvero <span class="math-span">\( C_{n, k}\)</span>, ovvero <span class="math-span">\( \left( \begin{array}{c} n \\ k \end{array} \right)\)</span>;         </li><li class="list-item">la probabilità che <span class="math-span">\( A\)</span> si verifichi <span class="math-span">\( k\)</span> volte, ovvero <span class="math-span">\( p^k\)</span>;         </li><li class="list-item">la probabilità che <span class="math-span">\( A\)</span> non si verifichi <span class="math-span">\( n - k\)</span> volte, ovvero <span class="math-span">\( (1 - p)^{n - k} = q^{n - k}\)</span>;     </li></ul>      ovvero      <span class="math-block">\[         p(X = k) = \left( \begin{array}{c} n \\ k \end{array} \right)  \cdot p^k \cdot q^{n - k}     \]</span><span class="inner-title">Valore atteso</span>     Il valore atteso di una v. c. binomiale di parametri <span class="math-span">\( n\)</span> e <span class="math-span">\( k\)</span> si ha che sarebbe uguale a     <span class="math-block">\[         \begin{array}{ccl}             E[X] &amp; = &amp; \sum_{k = 0}^n k \cdot p(X = k) \\             &amp; = &amp; \sum_{k = 0}^n k \cdot C_{n, k} \cdot p^k \cdot q^{n - k}         \end{array}     \]</span>     che risulta essere un calcolo abbastanza ostico.     <br/>     È però possibile considerare che <span class="math-span">\( X\)</span> sia uguale alla somma di <span class="math-span">\( n\)</span> variabili casuali di Bernoulli che indicano se l'<span class="math-span">\( i\)</span>-esimo esperimento (degli <span class="math-span">\( n\)</span> totali) si è verificato, ovvero     <span class="math-block">\[         X = \sum_{i = 1}^n X_i \qquad \text{con} \ X_i \sim Be(p)       \]</span>     Considerato ciò, si ha che     <span class="math-block">\[         \begin{array}{ccl}             E[X] &amp; = &amp; E\left[\sum_{i = 1}^n X_i \right] \\             &amp; = &amp; \sum_{i = 1}^n E[X_i] \\             &amp; = &amp; n \cdot E[X_i] \\             &amp; = &amp; n \cdot p         \end{array}     \]</span><span class="inner-title">Varianza</span>     La varianza di una v. c. binomiale di parametri <span class="math-span">\( n\)</span> e <span class="math-span">\( k\)</span>, considerando che <span class="math-span">\( X\)</span> è somma di <span class="math-span">\( n\)</span> variabili casuali di Bernoulli e che ogni esperimento è svolto in maniera indipendente per definizione, si ha che è uguale a     <span class="math-block">\[         \begin{array}{ccl}             Var(X) &amp; = &amp; Var\left( \sum_{i = 1}^n X_i \right) \\             &amp; \underset{\text{indip.}}{=} &amp; \sum_{i = 1}^n Var(X_i) \\             &amp; = &amp; n \cdot Var(X_i) \\             &amp; = &amp; n \cdot p \cdot q         \end{array}     \]</span><span class="inner-title">Funzione generatrice dei momenti</span>     La funzione generatrice dei momenti di una v. c. binomiale di parametri <span class="math-span">\( n\)</span> e <span class="math-span">\( p\)</span>, considerando che <span class="math-span">\( X\)</span> è somma di <span class="math-span">\( n\)</span> variabili casuali di Bernoulli e che ogni esperimento è svolto in maniera indipendente per definizione, si ha che è uguale a     <span class="math-block">\[         \begin{array}{ccl}             \phi_X(t) &amp; = &amp; E\left[ \mathrm{e}^{t \cdot X} \right] \\             &amp; = &amp; E\left[ \mathrm{e}^{t \cdot \sum_{i = 1}^n X_i} \right] \\             &amp; = &amp; E\left[ \prod_{i = 1}^n \mathrm{e}^{t \cdot X_i} \right] \\             &amp; \underset{\text{indip.}}{=} &amp; \prod_{i = 1}^n E\left[ \mathrm{e}^{t \cdot X_i} \right] \\             &amp; = &amp; \prod_{i = 1}^n \phi_{X_i}(t) \\              &amp; = &amp; \left( \phi_{X_i}(t) \right)^n \\             &amp; = &amp; \left( q + \mathrm{e}^{t} \cdot p  \right)^n         \end{array}     \]</span><div class="mynote environment"><h3 class="environment-title">Osservazioni personali - V. c. binomiale con <span class="math-span">\( n = 1\)</span> come v. c. di Bernoulli</h3><div class="environment-body">         Nel caso si abbia una variabile casuale binomiale di parametri <span class="math-span">\( 1\)</span> e <span class="math-span">\( p\)</span>, si ha che essa è una variabile casuale di Bernoulli di parametro <span class="math-span">\( p\)</span>, ovvero         <span class="math-block">\[             X \sim B(1, p) \sim Be(p)           \]</span></div></div></div></div><div class="demonstration environment" id="dem2-8"><h2 class="environment-title">Dimostrazione - Proprietà di riproducibilità di variabili casuali binomiali</h2><div class="environment-body">     Data la proposizione     <div class="proposition environment"><h3 class="environment-title">Enunciato</h3><div class="environment-body">         Considerando due variabili casuali binomiali indipendenti <span class="math-span">\( X\)</span> e <span class="math-span">\( Y\)</span>, tali che <span class="math-span">\( X \sim B(n, p)\)</span> e <span class="math-span">\( Y \sim B(m, p)\)</span> (ovvero con parametro <span class="math-span">\( p\)</span> uguale), si ha che         <span class="math-block">\[             X + Y \sim B(n + m, p)            \]</span></div></div><div class="proof environment"><h3 class="environment-title">Dimostrazione:<span class="material-symbols-outlined body-visibility-icon" onclick="toggle_proof(event)">visibility_off</span></h3><div class="environment-body hyde">         Per dimostrare questa proposizione, consideriamo         <span class="math-block">\[             Z = X + Y           \]</span>         Considerando le funzioni generatrici dei momenti, si ha che è necessario dimostrare che          <span class="math-block">\[             \phi_Z(t) = \phi_{X}(t) \cdot \phi_Y(t)            \]</span>         Si ha quindi che         <span class="math-block">\begin{aligned}             &amp; \phi_Z(t) = \phi_{X}(t) \cdot \phi_Y(t) &amp; \iff \\             &amp; \phi_Z(t) = E\left[ \mathrm{e}^{t \cdot X} \right] \cdot E\left[ \mathrm{e}^{t \cdot Y} \right] &amp; \iff         \end{aligned}</span>         e, considerando l'indipendenza delle due variabili, si ha che         <span class="math-block">\begin{aligned}             &amp; \phi_Z(t) = E\left[ \mathrm{e}^{t \cdot X} \cdot \mathrm{e}^{t \cdot Y} \right] &amp; \iff \\             &amp; \phi_Z(t) = E\left[ \mathrm{e}^{t \cdot X + t \cdot Y} \right] &amp; \iff \\             &amp; \phi_Z(t) = E\left[ \mathrm{e}^{t \cdot (X + Y)} \right] &amp; \iff \\             &amp; \phi_Z(t) = E\left[ \mathrm{e}^{t \cdot Z} \right] &amp;         \end{aligned}</span>         che dimostra la proposizione.     </div></div></div></div><div class="definition environment" id="def2-30"><h2 class="environment-title">Definizione - Variabile casuale geometrica</h2><div class="environment-body">     Una variabile casuale discreta <span class="math-span">\( X\)</span> si definisce geometrica di parametro <span class="math-span">\( p\)</span> (<span class="math-span">\( X \sim G(p)\)</span>) se, considerando il caso in cui si ripete un esperimento in maniera identica e indipendente (per infinite volte) e la probabilità che un certo esito <span class="math-span">\( A\)</span> si verifichi sia <span class="math-span">\( p\)</span>, <span class="math-span">\( X\)</span> rappresenta il <strong>numero di esperimenti eseguiti affinchè <span class="math-span">\( A\)</span> si sia verificato</strong>.     <br/>     Si ha quindi che <span class="math-span">\( X\)</span> è caratterizzata da un numero infinito di valori, ovvero     <span class="math-block">\[         X \in \{ 1, \ldots, +\infty \}       \]</span><span class="inner-title"><span class="math-span">\( P(X = k)\)</span></span>     Considerando un valore <span class="math-span">\( k \in \{ 1, \ldots, +\infty \}\)</span>, per calcolare <span class="math-span">\( P(X = k)\)</span> si ha che è necessario considerare che la probabilità che <span class="math-span">\( A\)</span> si verifichi al <span class="math-span">\( k\)</span>-esimo tentativo è data dalla probabilità che <span class="math-span">\( A\)</span> non si verifichi per <span class="math-span">\( k - 1\)</span> volte (ovvero <span class="math-span">\( q^{k - 1}\)</span>) e che si verifichi una volta (ovvero <span class="math-span">\( p\)</span>), per cui     <span class="math-block">\[         p(X = k) = q^{k - 1} \cdot p     \]</span><span class="inner-title">Probabilità totali</span>     Considerando il valore di una serie geometrica     <span class="math-block">\[         \sum_{k = 1}^{+\infty} r^k = \frac{1}{1 - r} \qquad \text{con} \ r \in ]0, 1[       \]</span>     Si ha che      <span class="math-block">\[         \begin{array}{ccl}             \sum_{k = 1}^{+\infty} p(X = k) &amp; = &amp; \sum_{k = 1}^{+\infty} q^{k - 1} \cdot p \\             &amp; = &amp; p \cdot \sum_{k = 1}^{+\infty} q^{k - 1}         \end{array}     \]</span>     e, considerando <span class="math-span">\( j = k - 1\)</span>, si ha che <span class="math-span">\( k = 1 \implies j = 0\)</span>, per cui     <span class="math-block">\[         \begin{array}{ccl}             \sum_{k = 1}^{+\infty} p(X = k) &amp; = &amp; p \cdot \sum_{j = 0}^{+\infty} q^{j} \\             &amp; = &amp; p \cdot \sum_{j = 0}^{+\infty} q^{j} \\             &amp; = &amp; p \cdot \frac{1}{1 - q} \\             &amp; = &amp; \frac{p}{1 - q} \\             &amp; = &amp; \frac{p}{p} \\              &amp; = &amp; 1         \end{array}     \]</span></div></div>
            </article>
            <nav class="buttons-container content-width">
                <a class="navigation-button previous" href="funzione-generatrice-di-momenti.html" rel="nofollow"><span>Funzione generatrice di momenti</span></a>
                <a class="navigation-button next" href="legge-dei-grandi-numeri.html" rel="nofollow"><span>Legge dei grandi numeri</span></a>
            </nav>
        </section>
        <div class="scroll-to-bottom-button" onclick="scroll_to_bottom()">
            <span class="material-symbols-outlined">
                keyboard_double_arrow_down
            </span>
        </div>
        <footer class="footer-wrapper">
            <div class="copyright-wrapper">
                <span> &copy; Copyright 2023</span> /
                <span>made by lorenzoarlo</span>
            </div>
            /
            <div class="privacy-wrapper">
                <span><a href="https://lorenzoarlo.github.io/privacy-and-cookies/" rel="nofollow">Pannello preferenze cookie</a></span> /
                <span><a href="https://lorenzoarlo.github.io/privacy-and-cookies/privacy-policy.html" rel="nofollow" target="_blank" >Privacy Policy</a></span>
            </div>
        </footer>
    </div>
</body>
</html>